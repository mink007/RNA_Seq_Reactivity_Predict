{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10dd24c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformer import Transformer \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "#import polars as pl\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "import fastai\n",
    "from fastai.vision.all import *\n",
    "\n",
    "import math\n",
    "import copy\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#device = \"cpu\"\n",
    "\n",
    "\n",
    "max_seq_len = 206 + 2  # +2 is added to cover start and end token of sequence This will change to 457 in private set  # Set the maximum sequence length to 100\n",
    "data_preprocessing_done = 0\n",
    "save_pkl = 0\n",
    "load_pkl = 0\n",
    "small_dataset = 0\n",
    "\n",
    "#Training Data\n",
    "if small_dataset == 1:\n",
    "    \n",
    "    #Uncomment this section when uploading the jupyter notebook\n",
    "    '''\n",
    "    df_read = pd.read_csv('/media/spartans/COMMON:/RNA_Project/train_data.csv')\n",
    "    \n",
    "    criteria_2A3 = (df_read['experiment_type'] == '2A3_MaP') & (df_read['SN_filter'] == 1)\n",
    "    criteria_DMS = (df_read['experiment_type'] == 'DMS_MaP') & (df_read['SN_filter'] == 1)\n",
    "\n",
    "    df_2A3_Filter1 = df_read[criteria_2A3]\n",
    "    df_2A3_Filter1 = df_2A3_Filter1.head(4)\n",
    "\n",
    "    df_DMS_Filter1 = df_read[criteria_DMS]\n",
    "    df_DMS_Filter1 = df_DMS_Filter1.head(4)\n",
    "\n",
    "   \n",
    "    df = pd.concat([df_2A3_Filter1, df_DMS_Filter1], ignore_index=True)\n",
    "    \n",
    "    df.to_csv('2A3_DMS_With_Filter1_Train_Data.csv', index=False)\n",
    "    #df_2A3_Filter1.to_csv('2A3_DMS_With_Filter1_Train_Data.csv', index=False)\n",
    "    '''\n",
    "    df = pd.read_csv('2A3_DMS_With_Filter1_Train_Data.csv')\n",
    "    #df = pd.read_csv('DMS_Data.csv')\n",
    "    #df = pd.read_csv('2A3_Data.csv')\n",
    "else:\n",
    "    df = pd.read_csv('/media/spartans/COMMON:/RNA_Project/train_data.csv')\n",
    "    \n",
    "\n",
    "#test_df = pd.read_csv('/media/spartans/COMMON:/RNA_Project/test_sequences.csv')\n",
    "\n",
    "\n",
    "seed=69\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = str(1)\n",
    "\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = str(1)\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2deb452f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>experiment_type</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>reads</th>\n",
       "      <th>signal_to_noise</th>\n",
       "      <th>SN_filter</th>\n",
       "      <th>reactivity_0001</th>\n",
       "      <th>reactivity_0002</th>\n",
       "      <th>reactivity_0003</th>\n",
       "      <th>...</th>\n",
       "      <th>reactivity_error_0197</th>\n",
       "      <th>reactivity_error_0198</th>\n",
       "      <th>reactivity_error_0199</th>\n",
       "      <th>reactivity_error_0200</th>\n",
       "      <th>reactivity_error_0201</th>\n",
       "      <th>reactivity_error_0202</th>\n",
       "      <th>reactivity_error_0203</th>\n",
       "      <th>reactivity_error_0204</th>\n",
       "      <th>reactivity_error_0205</th>\n",
       "      <th>reactivity_error_0206</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8cdfeef009ea</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAACGUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCCACGCGGGUCCUUCGGGACCCGCAAAAGAAACAACAACAACAAC</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>15k_2A3</td>\n",
       "      <td>2343</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51e61fbde94d</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAACAUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCACAGCGCUGGGUUCGCCCAGCGCAAAAGAAACAACAACAACAAC</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>15k_2A3</td>\n",
       "      <td>5326</td>\n",
       "      <td>1.933</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25ce8d5109cd</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAACCUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCAUGGGUCCUCCUUCGGGAGGACCAAAAGAAACAACAACAACAAC</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>15k_2A3</td>\n",
       "      <td>4647</td>\n",
       "      <td>2.347</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>07dcfb6d1965</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAACUUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCCAGGUAUUGACUUCGGUCAAUACAAAAGAAACAACAACAACAAC</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>15k_2A3</td>\n",
       "      <td>102843</td>\n",
       "      <td>11.824</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e561cc042a4c</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAACGAUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCAUGCAUGCGGCUUCGGCCGCAUGAAAAGAAACAACAACAACAAC</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>15k_2A3</td>\n",
       "      <td>7665</td>\n",
       "      <td>3.519</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643675</th>\n",
       "      <td>7951fb2f47f1</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGAGCGUCGUGUCUCUUGUACGUCUCGGUCACAAUACACGGUUUCGUCCGGUGCGUGGCAAUUCGGGGCACAUCAUGUUUUUCGUGCCUGGUGUGACCGCGCAAGGUGCGCGCGGUACGUAUCGAGCAGCGCUCCAAAACUCGCAACUUCGGUUGCGAGAAAAGAAACAACAACAACAAC</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>SL5_M2seq_DMS</td>\n",
       "      <td>37530</td>\n",
       "      <td>7.248</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643676</th>\n",
       "      <td>e0dc5823e5e1</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGAGCGUCGUGUCUCUUGUACGUCUCGGUCACAAUACACGGUUUCGUCCGGUGCGUGGCAAUUCGGGGCACAUCAUGUUUUUCGUGUCUGGUGUGACCGCGCAAGGUGCGCGCGGUACGUAUCGAGCAGCGCUCCAAAAACAUCGGAUUCGUCCGAUGUAAAAGAAACAACAACAACAAC</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>SL5_M2seq_DMS</td>\n",
       "      <td>337248</td>\n",
       "      <td>17.902</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643677</th>\n",
       "      <td>0d6036529b42</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGAGCGUCGUGUCUCUUGUACGUCUCGGUCACAAUACACGGUUUCGUCCGGUGCGUGGCAAUUCGGGGCACAUCAUGUUUUUCGUGGAUGGUGUGACCGCGCAAGGUGCGCGCGGUACGUAUCGAGCAGCGCUCCAAAAACCCCGGUUUCGACCGGGGUAAAAGAAACAACAACAACAAC</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>SL5_M2seq_DMS</td>\n",
       "      <td>44053</td>\n",
       "      <td>6.700</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643678</th>\n",
       "      <td>46d1f07d723b</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGAGCGUCGUGUCUCUUGUACGUCUCGGUCACAAUACACGGUUUCGUCCGGUGCGUGGCAAUUCGGGGCACAUCAUGUUUUUCGUGGGUGGUGUGACCGCGCAAGGUGCGCGCGGUACGUAUCGAGCAGCGCUCCAAAAGAGCUAUCUUCGGAUAGCUCAAAAGAAACAACAACAACAAC</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>SL5_M2seq_DMS</td>\n",
       "      <td>108600</td>\n",
       "      <td>11.716</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643679</th>\n",
       "      <td>60db72cc009f</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGAGCGUCGUGUCUCUUGUACGUCUCGGUCACAAUACACGGUUUCGUCCGGUGCGUGGCAAUUCGGGGCACAUCAUGUUUUUCGUGGUUGGUGUGACCGCGCAAGGUGCGCGCGGUACGUAUCGAGCAGCGCUCCAAAAGUAAGCACUUCGGUGCUUACAAAAGAAACAACAACAACAAC</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>SL5_M2seq_DMS</td>\n",
       "      <td>107394</td>\n",
       "      <td>10.888</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1643680 rows × 419 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          sequence_id  \\\n",
       "0        8cdfeef009ea   \n",
       "1        51e61fbde94d   \n",
       "2        25ce8d5109cd   \n",
       "3        07dcfb6d1965   \n",
       "4        e561cc042a4c   \n",
       "...               ...   \n",
       "1643675  7951fb2f47f1   \n",
       "1643676  e0dc5823e5e1   \n",
       "1643677  0d6036529b42   \n",
       "1643678  46d1f07d723b   \n",
       "1643679  60db72cc009f   \n",
       "\n",
       "                                                                                                                                                                                                               sequence  \\\n",
       "0                                            GGGAACGACUCGAGUAGAGUCGAAAAACGUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCCACGCGGGUCCUUCGGGACCCGCAAAAGAAACAACAACAACAAC   \n",
       "1                                            GGGAACGACUCGAGUAGAGUCGAAAAACAUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCACAGCGCUGGGUUCGCCCAGCGCAAAAGAAACAACAACAACAAC   \n",
       "2                                            GGGAACGACUCGAGUAGAGUCGAAAAACCUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCAUGGGUCCUCCUUCGGGAGGACCAAAAGAAACAACAACAACAAC   \n",
       "3                                            GGGAACGACUCGAGUAGAGUCGAAAAACUUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCCAGGUAUUGACUUCGGUCAAUACAAAAGAAACAACAACAACAAC   \n",
       "4                                            GGGAACGACUCGAGUAGAGUCGAAAAACGAUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCAUGCAUGCGGCUUCGGCCGCAUGAAAAGAAACAACAACAACAAC   \n",
       "...                                                                                                                                                                                                                 ...   \n",
       "1643675  GGGAACGACUCGAGUAGAGUCGAAAAGGAGCGUCGUGUCUCUUGUACGUCUCGGUCACAAUACACGGUUUCGUCCGGUGCGUGGCAAUUCGGGGCACAUCAUGUUUUUCGUGCCUGGUGUGACCGCGCAAGGUGCGCGCGGUACGUAUCGAGCAGCGCUCCAAAACUCGCAACUUCGGUUGCGAGAAAAGAAACAACAACAACAAC   \n",
       "1643676  GGGAACGACUCGAGUAGAGUCGAAAAGGAGCGUCGUGUCUCUUGUACGUCUCGGUCACAAUACACGGUUUCGUCCGGUGCGUGGCAAUUCGGGGCACAUCAUGUUUUUCGUGUCUGGUGUGACCGCGCAAGGUGCGCGCGGUACGUAUCGAGCAGCGCUCCAAAAACAUCGGAUUCGUCCGAUGUAAAAGAAACAACAACAACAAC   \n",
       "1643677  GGGAACGACUCGAGUAGAGUCGAAAAGGAGCGUCGUGUCUCUUGUACGUCUCGGUCACAAUACACGGUUUCGUCCGGUGCGUGGCAAUUCGGGGCACAUCAUGUUUUUCGUGGAUGGUGUGACCGCGCAAGGUGCGCGCGGUACGUAUCGAGCAGCGCUCCAAAAACCCCGGUUUCGACCGGGGUAAAAGAAACAACAACAACAAC   \n",
       "1643678  GGGAACGACUCGAGUAGAGUCGAAAAGGAGCGUCGUGUCUCUUGUACGUCUCGGUCACAAUACACGGUUUCGUCCGGUGCGUGGCAAUUCGGGGCACAUCAUGUUUUUCGUGGGUGGUGUGACCGCGCAAGGUGCGCGCGGUACGUAUCGAGCAGCGCUCCAAAAGAGCUAUCUUCGGAUAGCUCAAAAGAAACAACAACAACAAC   \n",
       "1643679  GGGAACGACUCGAGUAGAGUCGAAAAGGAGCGUCGUGUCUCUUGUACGUCUCGGUCACAAUACACGGUUUCGUCCGGUGCGUGGCAAUUCGGGGCACAUCAUGUUUUUCGUGGUUGGUGUGACCGCGCAAGGUGCGCGCGGUACGUAUCGAGCAGCGCUCCAAAAGUAAGCACUUCGGUGCUUACAAAAGAAACAACAACAACAAC   \n",
       "\n",
       "        experiment_type   dataset_name   reads  signal_to_noise  SN_filter  \\\n",
       "0               2A3_MaP        15k_2A3    2343            0.944          0   \n",
       "1               2A3_MaP        15k_2A3    5326            1.933          1   \n",
       "2               2A3_MaP        15k_2A3    4647            2.347          1   \n",
       "3               2A3_MaP        15k_2A3  102843           11.824          1   \n",
       "4               2A3_MaP        15k_2A3    7665            3.519          1   \n",
       "...                 ...            ...     ...              ...        ...   \n",
       "1643675         DMS_MaP  SL5_M2seq_DMS   37530            7.248          1   \n",
       "1643676         DMS_MaP  SL5_M2seq_DMS  337248           17.902          1   \n",
       "1643677         DMS_MaP  SL5_M2seq_DMS   44053            6.700          1   \n",
       "1643678         DMS_MaP  SL5_M2seq_DMS  108600           11.716          1   \n",
       "1643679         DMS_MaP  SL5_M2seq_DMS  107394           10.888          1   \n",
       "\n",
       "         reactivity_0001  reactivity_0002  reactivity_0003  ...  \\\n",
       "0                    NaN              NaN              NaN  ...   \n",
       "1                    NaN              NaN              NaN  ...   \n",
       "2                    NaN              NaN              NaN  ...   \n",
       "3                    NaN              NaN              NaN  ...   \n",
       "4                    NaN              NaN              NaN  ...   \n",
       "...                  ...              ...              ...  ...   \n",
       "1643675              NaN              NaN              NaN  ...   \n",
       "1643676              NaN              NaN              NaN  ...   \n",
       "1643677              NaN              NaN              NaN  ...   \n",
       "1643678              NaN              NaN              NaN  ...   \n",
       "1643679              NaN              NaN              NaN  ...   \n",
       "\n",
       "         reactivity_error_0197  reactivity_error_0198  reactivity_error_0199  \\\n",
       "0                          NaN                    NaN                    NaN   \n",
       "1                          NaN                    NaN                    NaN   \n",
       "2                          NaN                    NaN                    NaN   \n",
       "3                          NaN                    NaN                    NaN   \n",
       "4                          NaN                    NaN                    NaN   \n",
       "...                        ...                    ...                    ...   \n",
       "1643675                    NaN                    NaN                    NaN   \n",
       "1643676                    NaN                    NaN                    NaN   \n",
       "1643677                    NaN                    NaN                    NaN   \n",
       "1643678                    NaN                    NaN                    NaN   \n",
       "1643679                    NaN                    NaN                    NaN   \n",
       "\n",
       "         reactivity_error_0200  reactivity_error_0201  reactivity_error_0202  \\\n",
       "0                          NaN                    NaN                    NaN   \n",
       "1                          NaN                    NaN                    NaN   \n",
       "2                          NaN                    NaN                    NaN   \n",
       "3                          NaN                    NaN                    NaN   \n",
       "4                          NaN                    NaN                    NaN   \n",
       "...                        ...                    ...                    ...   \n",
       "1643675                    NaN                    NaN                    NaN   \n",
       "1643676                    NaN                    NaN                    NaN   \n",
       "1643677                    NaN                    NaN                    NaN   \n",
       "1643678                    NaN                    NaN                    NaN   \n",
       "1643679                    NaN                    NaN                    NaN   \n",
       "\n",
       "         reactivity_error_0203  reactivity_error_0204  reactivity_error_0205  \\\n",
       "0                          NaN                    NaN                    NaN   \n",
       "1                          NaN                    NaN                    NaN   \n",
       "2                          NaN                    NaN                    NaN   \n",
       "3                          NaN                    NaN                    NaN   \n",
       "4                          NaN                    NaN                    NaN   \n",
       "...                        ...                    ...                    ...   \n",
       "1643675                    NaN                    NaN                    NaN   \n",
       "1643676                    NaN                    NaN                    NaN   \n",
       "1643677                    NaN                    NaN                    NaN   \n",
       "1643678                    NaN                    NaN                    NaN   \n",
       "1643679                    NaN                    NaN                    NaN   \n",
       "\n",
       "         reactivity_error_0206  \n",
       "0                          NaN  \n",
       "1                          NaN  \n",
       "2                          NaN  \n",
       "3                          NaN  \n",
       "4                          NaN  \n",
       "...                        ...  \n",
       "1643675                    NaN  \n",
       "1643676                    NaN  \n",
       "1643677                    NaN  \n",
       "1643678                    NaN  \n",
       "1643679                    NaN  \n",
       "\n",
       "[1643680 rows x 419 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#criteria_2A3 = (df_read['experiment_type'] == '2A3_MaP') & (df_read['SN_filter'] == '1')\n",
    "#criteria_DMS = (df_read['experiment_type'] == 'DMS_MaP') & (df_read['SN_filter'] == '1')\n",
    "\n",
    "#criteria_2A3 = ((df_read['experiment_type'] == 'DMS_MaP')  & df_read['SN_filter'] == 1 )\n",
    "#df_2A3_Filter1 = df_read[criteria_2A3]\n",
    "#df_2A3_Filter1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "166c7518",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        #print(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4cee06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward1(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "    \n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        #print(f\"x after first linear layer: {x.size()}\")\n",
    "        x = self.relu(x)\n",
    "        #print(f\"x after activation: {x.size()}\")\n",
    "        x = self.dropout(x)\n",
    "        #print(f\"x after dropout: {x.size()}\")\n",
    "        x = self.linear2(x)\n",
    "        #print(f\"x after 2nd linear layer: {x.size()}\")\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c2bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        #print(self.pe[:, :x.size(1)].size())\n",
    "        #print((x + self.pe[:, :x.size(1)]).size())\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "    \n",
    "\n",
    "class PositionalEncoding1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        print(\"PE size\", PE.size())\n",
    "        print(\"x size\", x.size())\n",
    "        print(\"PE x size\", PE[:, :x.size(1)].size())\n",
    "        print(\"stacked dim\", stacked.size())\n",
    "        return x + PE[:, :x.size(1)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aff55ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d0f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b1354f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc1 = nn.Linear(max_seq_length-2,max_seq_length-2)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(device)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        #print(\"forward \",src, src.size())\n",
    "        print(\"forward \", tgt, tgt.size())\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        #print(\"Src_embedded size\", src_embedded.size())\n",
    "        #print(\"tgt size \", tgt.size())\n",
    "        #print(\"TGT  = \", tgt)\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "        #print(\"tgt_embedded size\", tgt_embedded.size())\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # Expand the dimensions to create an initial input for the decoder\n",
    "        #x = enc_output.unsqueeze(1).expand(-1, self.output_sequence_length, -1)\n",
    "\n",
    "\n",
    "        #output = self.fc(dec_output)\n",
    "        #print(\"Size OUTPUT\",output.size())\n",
    "        \n",
    "        xx = self.gelu(self.fc(dec_output))\n",
    "        print(\"Size xx\", xx.size())\n",
    "        final_out = self.fc1(xx)\n",
    "        print(\"Size final_out\", final_out.size())\n",
    "        return final_out\n",
    "        #include here FC layers with output dimension = input dimension of the sequence (considering padding this will be const number)\n",
    "        #return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61508b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#where reactivity is NaN,\n",
    "# have Nan replaced with average of all numbers in the sequence.\n",
    "\n",
    "#Map sequence letters to numbers.\n",
    "\n",
    "#Looking at Reads and Signal To Noise ..they appear to be somewhat coorelated.\n",
    "\n",
    "#Create Dataset function\n",
    "\n",
    "\n",
    "#In contrastive loss model... you can train based on#\n",
    "#in a sequence which part is 2D fold and which part is 3D fold\n",
    "#See notebook https://www.kaggle.com/code/something4kag/ribonanza-3d-coords-prep\n",
    "#if it can be helpful\n",
    "# Also analyze and which position in the sequence the fold can occur and which fold ended up happening based on the data\n",
    "\n",
    "#another method to formulate contrastive learning model is by deferentiating sequences with low SNR and hence reactivity value will not be with high confidence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db2f1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#In RNA, the most common base pairings you'll find are between the following nucleotide bases:\n",
    "\n",
    "#Adenine (A) and Uracil (U): A forms base pairs with U. This is a fundamental pairing and is commonly seen in RNA molecules, particularly in single-stranded regions. It is important for the stability of stem-loop structures and is a key component of RNA secondary structure.\n",
    "\n",
    "#Guanine (G) and Cytosine (C): G forms base pairs with C, just as it does in DNA. This pairing is less common in RNA secondary structure but is still important for certain RNA molecules. It may be more prevalent in the context of ribozymes and catalytic RNA.\n",
    "\n",
    "#While A-U and G-C are the primary base pairings in RNA, it's essential to understand that RNA can also exhibit non-canonical or non-standard base pairings, especially in more complex RNA structures. These non-canonical pairings can involve different combinations of A, U, G, and C and are often seen in tertiary structures or specialized RNA molecules with specific functions.\n",
    "\n",
    "#The prevalence of A-U and G-C base pairings in an RNA molecule can vary depending on its sequence and function. For instance, regions that need to form stable secondary structures often rely on A-U pairings, while regions involved in catalytic activities may include G-C pairs. RNA structures are diverse and can exhibit a wide range of base pairing interactions to achieve their biological functions.\n",
    "\n",
    "\n",
    "\n",
    "#So a contrastive loss can be formed here where\n",
    "#AU pair GC pair are strong positive - stable\n",
    "#AG, UC are less stable\n",
    "#UG, UA, CG pair are unstable so negative .\n",
    "#\n",
    "#In RNA, the stable base pairs among the four nucleotide bases (A, G, U, and C) follow the standard Watson-Crick base-pairing rules. Here are the stable base pairs among these bases:\n",
    "\n",
    "#GC pairs are more stable than AU pair\n",
    "\n",
    "\n",
    "#So you can tokenzie these pairs in the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5b71bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formulate Convolution NN or Dense Net which takes SNR, Reactivity error and feeds into the final layer of Transformer network\n",
    "# As an example\n",
    "#Lowest Reactivity Error and High SNR are  strong positives\n",
    "# Lowest SNR are strong negative.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#sequence length range from 115 to 206 in train dataset.\n",
    "#in final it will range from 207 to 457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dbb5976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In positional encoding\n",
    "#pos means the position of the word in the sequence.\n",
    "#i means the index for that particular word vector (index of the dimension)\n",
    "#dmodel is the dimension for e.g 512 from the example\n",
    "\n",
    "\n",
    "#For 2 experiment types i.e. DMS and 2A3 you can have start and end  token in a sequence for which data is available.\n",
    "#.e. start_DMS, end_DMS, start_2A3, end_2A3.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8607779c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cb051b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_247140/3653553058.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  react_columns = react_columns.applymap(lambda x: 0 if isinstance(x, (float, int)) and x < 0 else x)\n"
     ]
    }
   ],
   "source": [
    "if data_preprocessing_done == 0:\n",
    "    #Assigning reactivity Columns to react_columns dataframe and replacing NaN with 0.000 \n",
    "\n",
    "    react_columns = df.iloc[:,7:213].fillna(0.000000)\n",
    "\n",
    "    #Find mean of reactivity per row\n",
    "    react_columns = react_columns.applymap(lambda x: 0 if isinstance(x, (float, int)) and x < 0 else x)\n",
    "\n",
    "    react_columns_mean = react_columns.mean(axis=1)\n",
    "    #Free up memory\n",
    "    del(react_columns)\n",
    "\n",
    "    #Now replace NaN in react_columns with mean_reactivity\n",
    "    #react_columns_New = df.iloc[:,7:213].fillna(mean_reactivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6bd190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  37%|██████████████████████████▍                                             | 603760/1643680 [01:23<02:25, 7147.57it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "if data_preprocessing_done == 0:\n",
    "    react_columns = df.iloc[:,7:213] #You may have to capture those indices as per sequence length i.e. when it extends to 457\n",
    "\n",
    "    # Replace NaN values in each row with values from replace_values array\n",
    "    for index, row in tqdm(react_columns.iterrows(), total=len(react_columns), desc=\"Processing rows\"):\n",
    "        react_columns.loc[index] = row.fillna(react_columns_mean[index])\n",
    "    \n",
    "    react_columns = react_columns.applymap(lambda x: 0 if isinstance(x, (float, int)) and x < 0 else x)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_map = {'A':1.0,\n",
    "           'C':2.0,\n",
    "           'G':3.0,\n",
    "           'U':4.0, \n",
    "           'M':5.0,  #Start token of DMS exp seq\n",
    "           'N':6.0,  #End token of DMS exp seq\n",
    "           'T':7.0,  #Start token of 2A3 exp seq\n",
    "           'X':8.0,  #End token of 2A3 exp seq\n",
    "           'Z':0.0   #Padding token\n",
    "          }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57653080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over rows and replace a specific value in all columns\n",
    "\n",
    "\n",
    "#max_seq_len = 206 #This will change to 457 in private set  # Set the maximum sequence length to 100\n",
    "if data_preprocessing_done == 0:\n",
    "    df_sq = copy.deepcopy(df['sequence']) #saving a backup of sequences.\n",
    "    #react_columns['maped_sequence'] = [' ']*len(react_columns)\n",
    "    #for index, row in tqdm(df.iterrows(), total=len(df.rows), desc=\"Processing rows\"):\n",
    "    for index, row in tqdm(react_columns.iterrows(), total=len(react_columns), desc=\"Processing rows\"):\n",
    "        if df.at[index, 'experiment_type'] == \"2A3_MaP\":\n",
    "            concat_seq = 'T' + df.at[index, 'sequence'] + 'X'\n",
    "        if df.at[index, 'experiment_type'] == \"DMS_MaP\":\n",
    "            concat_seq = 'M' + df.at[index, 'sequence'] + 'N'\n",
    "        padding = \"Z\"*(max_seq_len - 2 - len(df.at[index, 'sequence']))\n",
    "        concatenated_seq = concat_seq + padding\n",
    "        df.at[index, 'sequence'] = [seq_map[s] for s in concatenated_seq]\n",
    "        #react_columns.at[index, 'maped_sequence'] = [seq_map[s] for s in concatenated_seq]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2e3e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if data_preprocessing_done == 0:\n",
    "    #react_columns['reactivity_0207'] = copy.deepcopy(react_columns['reactivity_0206'])\n",
    "    #react_columns['reactivity_0208'] = copy.deepcopy(react_columns['reactivity_0206'])\n",
    "    react_columns['mapped_sequence'] = copy.deepcopy(df['sequence'])\n",
    "    #del react_columns['reactivity_0207']\n",
    "    #del react_columns['reactivity_0208']\n",
    "if save_pkl == 1:\n",
    "    react_columns.to_pickle(\"react_columns.pkl\")\n",
    "\n",
    "if load_pkl == 1:\n",
    "    react_columns = pd.read_pickle(\"react_columns.pkl\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d4f1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for your data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, max_seq_len):\n",
    "        self.data = data\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trg_sequence = torch.tensor(self.data.iloc[idx, :].tolist()[:-1])\n",
    "        src_sequence = torch.tensor(self.data.iloc[idx, :].tolist()[-1])\n",
    "        \n",
    "        #print(\"Size src seq \", src_sequence.size())\n",
    "        #print(\"Size trg seq \", trg_sequence, trg_sequence.size())\n",
    "        \n",
    "        return src_sequence, trg_sequence\n",
    "\n",
    "    \n",
    "# Create a DataLoader for batching and shuffling\n",
    "batch_size = 64\n",
    "#batch_size = 3\n",
    "\n",
    "\n",
    "# Define the size of the training set\n",
    "dataset_size = len(react_columns)\n",
    "train_size = int(0.5 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "custom_dataset = CustomDataset(react_columns, max_seq_len)\n",
    "train_indices, test_indices = random_split(custom_dataset, [train_size, test_size])\n",
    "train_dataloader = DataLoader(train_indices, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_indices, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a0751",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(device)\n",
    "\n",
    "\n",
    "class MyTransformerEncoderLayer(nn.TransformerEncoderLayer):\n",
    "    def __init__(self, d_model, nhead, batch_first=True, **kwargs):\n",
    "        super(MyTransformerEncoderLayer, self).__init__(d_model, nhead, **kwargs)\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, batch_first=batch_first)\n",
    "\n",
    "\n",
    "class TransformerRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, nhead, num_layers, output_sequence_length):\n",
    "        super(TransformerRegressionModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        '''\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        '''\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            MyTransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, batch_first=True),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=input_dim, nhead=nhead),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(input_dim, output_sequence_length * 1)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.output_sequence_length = output_sequence_length\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):       \n",
    "        # Apply embedding to input\n",
    "        #print(\"X shape 1\", x.shape)\n",
    "        x = self.embedding(x)\n",
    "        #print(\"X shape 2\", x.shape)\n",
    "        # Permute to (sequence_length, batch_size, input_dim)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        #print(\"X shape 3\", x.shape)\n",
    "        \n",
    "        # Encoder forward pass\n",
    "        encoder_output = self.transformer_encoder(x)\n",
    "\n",
    "        # Decoder forward pass with memory from the encoder\n",
    "        decoder_output = self.transformer_decoder(x, memory=encoder_output)\n",
    "\n",
    "        # Global average pooling over the sequence dimension\n",
    "        x = decoder_output.mean(dim=0)\n",
    "\n",
    "        # Final fully connected layer with GELU activation\n",
    "        x = self.gelu(self.fc(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Hyperparameters\n",
    "input_dim =  max_seq_len # Adjust based on your input size\n",
    "hidden_dim = 208\n",
    "nhead = 4\n",
    "num_layers = 3\n",
    "output_sequence_length = max_seq_len - 2  # Adjust based on your output sequence length\n",
    "#batch_size = 1\n",
    "num_epochs = 2\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = TransformerRegressionModel(input_dim, hidden_dim, nhead, num_layers, output_sequence_length).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9, weight_decay=0, amsgrad=False)\n",
    "\n",
    "\n",
    "\n",
    "training_losses = []  # to store training losses\n",
    "validation_losses = []\n",
    "prev_average_val_loss = 10000000000\n",
    "list_of_avg_loss = []\n",
    "epochss = []\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0  # accumulate loss for the entire epoch\n",
    "    val_loss = 0.0\n",
    "    #for inputs, targets in tqdm(train_dataloader.iterrows(), total=len(train_dataloader), desc=\"Processing rows\"):\n",
    "    #with tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as tqdm_loader:\n",
    "    #    for inputs, targets in tqdm_loader:\n",
    "         \n",
    "    model.train()        \n",
    "    #for batch, dat in enumerate(train_dataloader, 1): \n",
    "    for batch, dat in enumerate(tqdm(train_dataloader, desc=\"Processing batches\", unit=\"batch\"), 1):\n",
    "            inputs = dat[0].to(device, dtype=torch.long)\n",
    "            targets = dat[1].to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            #print(f'TRAIN:  Batch {batch}, Input: {inputs.shape}, Outputs: {outputs.shape}, Targets: {targets.shape}')\n",
    "            outputs[outputs < 0] = 0.0\n",
    "            outputs[outputs > 1] = 1.0\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            #print(f\"Epoch: {epoch+1}, Training Loss: {loss.item()}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        #for batch, dat in enumerate(test_dataloader, 1):\n",
    "        for batch, dat in enumerate(tqdm(test_dataloader, desc=\"Processing batches\", unit=\"batch\"), 1):\n",
    "            inputs = dat[0].to(device, dtype=torch.long)\n",
    "            targets = dat[1].to(device).float()\n",
    "            output = model(inputs)\n",
    "            #print(f'VAL:  Batch {batch}, Input: {inputs.shape}, Outputs: {output.shape}, Targets: {targets.shape}')\n",
    "            loss = criterion(output, targets)\n",
    "            print(f\"Epoch: {epoch+1}, Validation Loss: {loss.item()}\")\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    #outputs.requires_grad_(True)\n",
    "    \n",
    "    \n",
    "    average_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    training_losses.append(average_epoch_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {average_epoch_loss:.4f}')\n",
    "    \n",
    "    average_val_loss = val_loss / len(test_dataloader)\n",
    "    validation_losses.append(average_val_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {average_val_loss:.4f}')\n",
    "    epochss.append(epoch)\n",
    "    \n",
    "    list_of_avg_loss.append(average_val_loss)\n",
    "    #Save best model Every Epoch\n",
    "    if average_val_loss < prev_average_val_loss:\n",
    "        torch.save(model, \"Trained_Model.model\")\n",
    "        print(\"Model Saved because \")\n",
    "        print(f'Current average validation loss {average_val_loss:.4f} is less than previous average validation loss {prev_average_val_loss:.4f}')\n",
    "    prev_average_val_loss = min(list_of_avg_loss)\n",
    "\n",
    "\n",
    "# Plotting the training loss\n",
    "plt.plot(epochss, training_losses, label='Training Loss')\n",
    "plt.plot(epochss, validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# After training, you can use the model for prediction\n",
    "# Example usage:\n",
    "# test_input = torch.randn(1, sequence_length, input_dim)  # Adjust based on your input size and sequence length\n",
    "# predicted_output = model(test_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53570b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d418c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation predict\n",
    "#val_df = \n",
    "\n",
    "#Read TEST Sequence\n",
    "test_df = pd.read_csv('/media/spartans/COMMON:/RNA_Project/test_sequences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f6c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(4)\n",
    "test_df.at[0, 'sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df['sequence'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482b60c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['DMS_sequence'] = copy.deepcopy(test_df['sequence'])\n",
    "test_df['2A3_sequence'] = copy.deepcopy(test_df['sequence'])\n",
    "\n",
    "\n",
    "test_df_sq = copy.deepcopy(test_df['sequence']) #saving a backup of sequences.\n",
    "#react_columns['maped_sequence'] = [' ']*len(react_columns)\n",
    "#for index, row in tqdm(df.iterrows(), total=len(df.rows), desc=\"Processing rows\"):\n",
    "for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing rows\"):\n",
    "    \n",
    "    padding = \"Z\"*(max_seq_len - 2 - len(test_df.at[index, 'sequence']))\n",
    "    form_DMS_seq = 'M' + test_df.at[index, 'sequence'] + 'N' + padding\n",
    "    form_2A3_seq = 'T' + test_df.at[index, 'sequence'] + 'X' + padding\n",
    "    \n",
    "\n",
    "    test_df.at[index, 'DMS_sequence'] = [seq_map[s] for s in form_DMS_seq]\n",
    "    test_df.at[index, '2A3_sequence'] = [seq_map[s] for s in form_2A3_seq]\n",
    "    if small_dataset == 1:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb69870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.no_grad():\n",
    "#src_data = src_data.to(device=device)\n",
    "#tgt_data = tgt_data.to(device=device, dtype=torch.long)\n",
    "#output = transformer_model(src_data, tgt_data[:, :-1])\n",
    "\n",
    "#out = transformer_model(test_df['2A3_sequence'][0], tgt_data[:, :-1])\n",
    "\n",
    "input_sequence = torch.tensor([test_df.at[0, 'DMS_sequence']]).to(device=device, dtype=torch.long)\n",
    "#inputs = dat[0].to(device, dtype=torch.long)\n",
    "out = []\n",
    "print(\"input seq\", input_sequence.size())\n",
    "out = model(input_sequence)\n",
    "print(\"out\", out.size)\n",
    "out[out < 0] = 0.0\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e8a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['id_max'][0]\n",
    "\n",
    "\n",
    "for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing rows\"):\n",
    "    input_sequence = torch.tensor([test_df.at[index, 'DMS_sequence']])\n",
    "    seq_length = len(test_df.at[index, 'sequence'])\n",
    "    out_dms = []\n",
    "    out_dms = model(input_sequence).tolist()\n",
    "    out_dms[:seq_length]\n",
    "   \n",
    "    out_2a3 = []\n",
    "    input_sequence = torch.tensor([test_df.at[index, '2A3_sequence']])\n",
    "    out_2a3 = model(input_sequence).tolist()\n",
    "    out_2a3[:seq_length]\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6b15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = out.tolist()\n",
    "#test_df['id_min'][0]\n",
    "aa[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a96b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = test_df.shape[0]   - 1\n",
    "print(ii)\n",
    "#for ii in range(1643450, 1643670):\n",
    "    #ii = 1643680\n",
    "seq_length = len(test_df.at[ii, 'sequence'])\n",
    "print(seq_length, test_df['id_min'][ii], test_df['id_max'][ii],  test_df['id_max'][ii] - test_df['id_min'][ii])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf76a66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
