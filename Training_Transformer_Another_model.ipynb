{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10dd24c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformer import Transformer \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "#import polars as pl\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "import fastai\n",
    "from fastai.vision.all import *\n",
    "\n",
    "import math\n",
    "import copy\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "max_seq_len = 206 + 2  # +2 is added to cover start and end token of sequence This will change to 457 in private set  # Set the maximum sequence length to 100\n",
    "data_preprocessing_done = 0\n",
    "save_pkl = 0\n",
    "load_pkl = 0\n",
    "small_dataset = 1\n",
    "\n",
    "#Training Data\n",
    "if small_dataset == 1:\n",
    "    \n",
    "    #Uncomment this section when uploading the jupyter notebook\n",
    "    '''\n",
    "    df_read = pd.read_csv('/media/spartans/COMMON:/RNA_Project/train_data.csv')\n",
    "    \n",
    "    criteria_2A3 = (df_read['experiment_type'] == '2A3_MaP') & (df_read['SN_filter'] == 1)\n",
    "    criteria_DMS = (df_read['experiment_type'] == 'DMS_MaP') & (df_read['SN_filter'] == 1)\n",
    "\n",
    "    df_2A3_Filter1 = df_read[criteria_2A3]\n",
    "    df_2A3_Filter1 = df_2A3_Filter1.head(4)\n",
    "\n",
    "    df_DMS_Filter1 = df_read[criteria_DMS]\n",
    "    df_DMS_Filter1 = df_DMS_Filter1.head(4)\n",
    "\n",
    "   \n",
    "    df = pd.concat([df_2A3_Filter1, df_DMS_Filter1], ignore_index=True)\n",
    "    \n",
    "    df.to_csv('2A3_DMS_With_Filter1_Train_Data.csv', index=False)\n",
    "    #df_2A3_Filter1.to_csv('2A3_DMS_With_Filter1_Train_Data.csv', index=False)\n",
    "    '''\n",
    "    df = pd.read_csv('2A3_DMS_With_Filter1_Train_Data.csv')\n",
    "    #df = pd.read_csv('DMS_Data.csv')\n",
    "    #df = pd.read_csv('2A3_Data.csv')\n",
    "else:\n",
    "    df = pd.read_csv('/media/spartans/COMMON:/RNA_Project/train_data.csv')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "seed=69\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = str(1)\n",
    "\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = str(1)\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2deb452f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>experiment_type</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>reads</th>\n",
       "      <th>signal_to_noise</th>\n",
       "      <th>SN_filter</th>\n",
       "      <th>reactivity_0001</th>\n",
       "      <th>reactivity_0002</th>\n",
       "      <th>reactivity_0003</th>\n",
       "      <th>...</th>\n",
       "      <th>reactivity_error_0197</th>\n",
       "      <th>reactivity_error_0198</th>\n",
       "      <th>reactivity_error_0199</th>\n",
       "      <th>reactivity_error_0200</th>\n",
       "      <th>reactivity_error_0201</th>\n",
       "      <th>reactivity_error_0202</th>\n",
       "      <th>reactivity_error_0203</th>\n",
       "      <th>reactivity_error_0204</th>\n",
       "      <th>reactivity_error_0205</th>\n",
       "      <th>reactivity_error_0206</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51e61fbde94d</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAACAUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCACAGCGCUGGGUUCGCCCAGCGCAAAAGAAACAACAACAACAAC</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>15k_2A3</td>\n",
       "      <td>5326</td>\n",
       "      <td>1.933</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25ce8d5109cd</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAACCUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCAUGGGUCCUCCUUCGGGAGGACCAAAAGAAACAACAACAACAAC</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>15k_2A3</td>\n",
       "      <td>4647</td>\n",
       "      <td>2.347</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07dcfb6d1965</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAACUUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCCAGGUAUUGACUUCGGUCAAUACAAAAGAAACAACAACAACAAC</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>15k_2A3</td>\n",
       "      <td>102843</td>\n",
       "      <td>11.824</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e561cc042a4c</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAACGAUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCAUGCAUGCGGCUUCGGCCGCAUGAAAAGAAACAACAACAACAAC</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>15k_2A3</td>\n",
       "      <td>7665</td>\n",
       "      <td>3.519</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25ce8d5109cd</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAACCUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCAUGGGUCCUCCUUCGGGAGGACCAAAAGAAACAACAACAACAAC</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>15k_DMS</td>\n",
       "      <td>1964</td>\n",
       "      <td>1.848</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>07dcfb6d1965</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAACUUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCCAGGUAUUGACUUCGGUCAAUACAAAAGAAACAACAACAACAAC</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>15k_DMS</td>\n",
       "      <td>45863</td>\n",
       "      <td>9.291</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>e561cc042a4c</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAACGAUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCAUGCAUGCGGCUUCGGCCGCAUGAAAAGAAACAACAACAACAAC</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>15k_DMS</td>\n",
       "      <td>6219</td>\n",
       "      <td>3.210</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aa948762535f</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAACGCUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCCAGGGUGUCACUUCGGUGACACCAAAAGAAACAACAACAACAAC</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>15k_DMS</td>\n",
       "      <td>7024</td>\n",
       "      <td>3.109</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 419 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sequence_id  \\\n",
       "0  51e61fbde94d   \n",
       "1  25ce8d5109cd   \n",
       "2  07dcfb6d1965   \n",
       "3  e561cc042a4c   \n",
       "4  25ce8d5109cd   \n",
       "5  07dcfb6d1965   \n",
       "6  e561cc042a4c   \n",
       "7  aa948762535f   \n",
       "\n",
       "                                                                                                                                                                     sequence  \\\n",
       "0  GGGAACGACUCGAGUAGAGUCGAAAAACAUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCACAGCGCUGGGUUCGCCCAGCGCAAAAGAAACAACAACAACAAC   \n",
       "1  GGGAACGACUCGAGUAGAGUCGAAAAACCUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCAUGGGUCCUCCUUCGGGAGGACCAAAAGAAACAACAACAACAAC   \n",
       "2  GGGAACGACUCGAGUAGAGUCGAAAAACUUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCCAGGUAUUGACUUCGGUCAAUACAAAAGAAACAACAACAACAAC   \n",
       "3  GGGAACGACUCGAGUAGAGUCGAAAAACGAUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCAUGCAUGCGGCUUCGGCCGCAUGAAAAGAAACAACAACAACAAC   \n",
       "4  GGGAACGACUCGAGUAGAGUCGAAAAACCUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCAUGGGUCCUCCUUCGGGAGGACCAAAAGAAACAACAACAACAAC   \n",
       "5  GGGAACGACUCGAGUAGAGUCGAAAAACUUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCCAGGUAUUGACUUCGGUCAAUACAAAAGAAACAACAACAACAAC   \n",
       "6  GGGAACGACUCGAGUAGAGUCGAAAAACGAUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCAUGCAUGCGGCUUCGGCCGCAUGAAAAGAAACAACAACAACAAC   \n",
       "7  GGGAACGACUCGAGUAGAGUCGAAAAACGCUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCCAGGGUGUCACUUCGGUGACACCAAAAGAAACAACAACAACAAC   \n",
       "\n",
       "  experiment_type dataset_name   reads  signal_to_noise  SN_filter  \\\n",
       "0         2A3_MaP      15k_2A3    5326            1.933          1   \n",
       "1         2A3_MaP      15k_2A3    4647            2.347          1   \n",
       "2         2A3_MaP      15k_2A3  102843           11.824          1   \n",
       "3         2A3_MaP      15k_2A3    7665            3.519          1   \n",
       "4         DMS_MaP      15k_DMS    1964            1.848          1   \n",
       "5         DMS_MaP      15k_DMS   45863            9.291          1   \n",
       "6         DMS_MaP      15k_DMS    6219            3.210          1   \n",
       "7         DMS_MaP      15k_DMS    7024            3.109          1   \n",
       "\n",
       "   reactivity_0001  reactivity_0002  reactivity_0003  ...  \\\n",
       "0              NaN              NaN              NaN  ...   \n",
       "1              NaN              NaN              NaN  ...   \n",
       "2              NaN              NaN              NaN  ...   \n",
       "3              NaN              NaN              NaN  ...   \n",
       "4              NaN              NaN              NaN  ...   \n",
       "5              NaN              NaN              NaN  ...   \n",
       "6              NaN              NaN              NaN  ...   \n",
       "7              NaN              NaN              NaN  ...   \n",
       "\n",
       "   reactivity_error_0197  reactivity_error_0198  reactivity_error_0199  \\\n",
       "0                    NaN                    NaN                    NaN   \n",
       "1                    NaN                    NaN                    NaN   \n",
       "2                    NaN                    NaN                    NaN   \n",
       "3                    NaN                    NaN                    NaN   \n",
       "4                    NaN                    NaN                    NaN   \n",
       "5                    NaN                    NaN                    NaN   \n",
       "6                    NaN                    NaN                    NaN   \n",
       "7                    NaN                    NaN                    NaN   \n",
       "\n",
       "   reactivity_error_0200  reactivity_error_0201  reactivity_error_0202  \\\n",
       "0                    NaN                    NaN                    NaN   \n",
       "1                    NaN                    NaN                    NaN   \n",
       "2                    NaN                    NaN                    NaN   \n",
       "3                    NaN                    NaN                    NaN   \n",
       "4                    NaN                    NaN                    NaN   \n",
       "5                    NaN                    NaN                    NaN   \n",
       "6                    NaN                    NaN                    NaN   \n",
       "7                    NaN                    NaN                    NaN   \n",
       "\n",
       "   reactivity_error_0203  reactivity_error_0204  reactivity_error_0205  \\\n",
       "0                    NaN                    NaN                    NaN   \n",
       "1                    NaN                    NaN                    NaN   \n",
       "2                    NaN                    NaN                    NaN   \n",
       "3                    NaN                    NaN                    NaN   \n",
       "4                    NaN                    NaN                    NaN   \n",
       "5                    NaN                    NaN                    NaN   \n",
       "6                    NaN                    NaN                    NaN   \n",
       "7                    NaN                    NaN                    NaN   \n",
       "\n",
       "   reactivity_error_0206  \n",
       "0                    NaN  \n",
       "1                    NaN  \n",
       "2                    NaN  \n",
       "3                    NaN  \n",
       "4                    NaN  \n",
       "5                    NaN  \n",
       "6                    NaN  \n",
       "7                    NaN  \n",
       "\n",
       "[8 rows x 419 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#criteria_2A3 = (df_read['experiment_type'] == '2A3_MaP') & (df_read['SN_filter'] == '1')\n",
    "#criteria_DMS = (df_read['experiment_type'] == 'DMS_MaP') & (df_read['SN_filter'] == '1')\n",
    "\n",
    "#criteria_2A3 = ((df_read['experiment_type'] == 'DMS_MaP')  & df_read['SN_filter'] == 1 )\n",
    "#df_2A3_Filter1 = df_read[criteria_2A3]\n",
    "#df_2A3_Filter1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "166c7518",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        #print(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4cee06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward1(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "    \n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        #print(f\"x after first linear layer: {x.size()}\")\n",
    "        x = self.relu(x)\n",
    "        #print(f\"x after activation: {x.size()}\")\n",
    "        x = self.dropout(x)\n",
    "        #print(f\"x after dropout: {x.size()}\")\n",
    "        x = self.linear2(x)\n",
    "        #print(f\"x after 2nd linear layer: {x.size()}\")\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c2bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        #print(self.pe[:, :x.size(1)].size())\n",
    "        #print((x + self.pe[:, :x.size(1)]).size())\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "    \n",
    "\n",
    "class PositionalEncoding1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        print(\"PE size\", PE.size())\n",
    "        print(\"x size\", x.size())\n",
    "        print(\"PE x size\", PE[:, :x.size(1)].size())\n",
    "        print(\"stacked dim\", stacked.size())\n",
    "        return x + PE[:, :x.size(1)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aff55ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d0f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b1354f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(device)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        #print(\"forward \",src, src.size())\n",
    "        #print(\"forward \", tgt, tgt.size())\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        print(\"Src_embedded size\", src_embedded.size())\n",
    "        print(\"tgt size \", tgt.size())\n",
    "        print(\"TGT  = \", tgt)\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "        print(\"tgt_embedded size\", tgt_embedded.size())\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        #include here FC layers with output dimension = input dimension of the sequence (considering padding this will be const number)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e6eb5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef7b613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#src_data\n",
    "#src_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddfe523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#src_data.size()\n",
    "#src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4f5478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61508b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#where reactivity is NaN,\n",
    "# have Nan replaced with average of all numbers in the sequence.\n",
    "\n",
    "#Map sequence letters to numbers.\n",
    "\n",
    "#Looking at Reads and Signal To Noise ..they appear to be somewhat coorelated.\n",
    "\n",
    "#Create Dataset function\n",
    "\n",
    "\n",
    "#In contrastive loss model... you can train based on#\n",
    "#in a sequence which part is 2D fold and which part is 3D fold\n",
    "#See notebook https://www.kaggle.com/code/something4kag/ribonanza-3d-coords-prep\n",
    "#if it can be helpful\n",
    "# Also analyze and which position in the sequence the fold can occur and which fold ended up happening based on the data\n",
    "\n",
    "#another method to formulate contrastive learning model is by deferentiating sequences with low SNR and hence reactivity value will not be with high confidence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db2f1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#In RNA, the most common base pairings you'll find are between the following nucleotide bases:\n",
    "\n",
    "#Adenine (A) and Uracil (U): A forms base pairs with U. This is a fundamental pairing and is commonly seen in RNA molecules, particularly in single-stranded regions. It is important for the stability of stem-loop structures and is a key component of RNA secondary structure.\n",
    "\n",
    "#Guanine (G) and Cytosine (C): G forms base pairs with C, just as it does in DNA. This pairing is less common in RNA secondary structure but is still important for certain RNA molecules. It may be more prevalent in the context of ribozymes and catalytic RNA.\n",
    "\n",
    "#While A-U and G-C are the primary base pairings in RNA, it's essential to understand that RNA can also exhibit non-canonical or non-standard base pairings, especially in more complex RNA structures. These non-canonical pairings can involve different combinations of A, U, G, and C and are often seen in tertiary structures or specialized RNA molecules with specific functions.\n",
    "\n",
    "#The prevalence of A-U and G-C base pairings in an RNA molecule can vary depending on its sequence and function. For instance, regions that need to form stable secondary structures often rely on A-U pairings, while regions involved in catalytic activities may include G-C pairs. RNA structures are diverse and can exhibit a wide range of base pairing interactions to achieve their biological functions.\n",
    "\n",
    "\n",
    "\n",
    "#So a contrastive loss can be formed here where\n",
    "#AU pair GC pair are strong positive - stable\n",
    "#AG, UC are less stable\n",
    "#UG, UA, CG pair are unstable so negative .\n",
    "#\n",
    "#In RNA, the stable base pairs among the four nucleotide bases (A, G, U, and C) follow the standard Watson-Crick base-pairing rules. Here are the stable base pairs among these bases:\n",
    "\n",
    "#GC pairs are more stable than AU pair\n",
    "\n",
    "\n",
    "#So you can tokenzie these pairs in the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5b71bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formulate Convolution NN or Dense Net which takes SNR, Reactivity error and feeds into the final layer of Transformer network\n",
    "# As an example\n",
    "#Lowest Reactivity Error and High SNR are  strong positives\n",
    "# Lowest SNR are strong negative.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#sequence length range from 115 to 206 in train dataset.\n",
    "#in final it will range from 207 to 457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dbb5976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In positional encoding\n",
    "#pos means the position of the word in the sequence.\n",
    "#i means the index for that particular word vector (index of the dimension)\n",
    "#dmodel is the dimension for e.g 512 from the example\n",
    "\n",
    "\n",
    "#For 2 experiment types i.e. DMS and 2A3 you can have start and end  token in a sequence for which data is available.\n",
    "#.e. start_DMS, end_DMS, start_2A3, end_2A3.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd9a8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6045989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.iloc[:,150:213]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8607779c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cb051b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10063/398611985.py:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  react_columns = react_columns.applymap(lambda x: 0 if isinstance(x, (float, int)) and x < 0 else x)\n"
     ]
    }
   ],
   "source": [
    "if data_preprocessing_done == 0:\n",
    "    #Assigning reactivity Columns to react_columns dataframe and replacing NaN with 0.000 \n",
    "\n",
    "    #del(mean_reactivity)\n",
    "    #del(react_columns_New)\n",
    "    #del(react_columns)\n",
    "    react_columns = df.iloc[:,7:213].fillna(0.000000)\n",
    "\n",
    "    #Find mean of reactivity per row\n",
    "    #df['mean'] = df.mean(axis=1)\n",
    "    #react_columns = react_columns.applymap(lambda x: 0.0 if x < 0.0 else x)\n",
    "    react_columns = react_columns.applymap(lambda x: 0 if isinstance(x, (float, int)) and x < 0 else x)\n",
    "\n",
    "    react_columns_mean = react_columns.mean(axis=1)\n",
    "    #Free up memory\n",
    "    del(react_columns)\n",
    "\n",
    "    #Now replace NaN in react_columns with mean_reactivity\n",
    "    #react_columns_New = df.iloc[:,7:213].fillna(mean_reactivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac6bd190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 5047.30it/s]\n",
      "/tmp/ipykernel_10063/1102462686.py:8: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  react_columns = react_columns.applymap(lambda x: 0 if isinstance(x, (float, int)) and x < 0 else x)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if data_preprocessing_done == 0:\n",
    "    react_columns = df.iloc[:,7:213] #You may have to capture those indices as per sequence length i.e. when it extends to 457\n",
    "\n",
    "    # Replace NaN values in each row with values from replace_values array\n",
    "    for index, row in tqdm(react_columns.iterrows(), total=len(react_columns), desc=\"Processing rows\"):\n",
    "        react_columns.loc[index] = row.fillna(react_columns_mean[index])\n",
    "    \n",
    "    react_columns = react_columns.applymap(lambda x: 0 if isinstance(x, (float, int)) and x < 0 else x)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d95559ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#react_columns_New = df.iloc[:,7:213].fillna(react_columns['mean_reactivity'])\n",
    "#react_columns_New\n",
    "#react_columns.columns\n",
    "\n",
    "\n",
    "#for column_name in tqdm(react_columns.columns, total=len(react_columns.columns), desc=\"Processing\"):\n",
    "#    df[column_name] = react_columns[column_name]\n",
    "    \n",
    "#react_columns['sequence'] = df['experiment_type'] + 'start' + df['sequence']\n",
    "#react_columns['sequence'] = react_columns['sequence'] + 'end' + df['experiment_type']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a10cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_map = {'A':1,\n",
    "           'C':2,\n",
    "           'G':3,\n",
    "           'U':4, \n",
    "           'M':5,  #Start token of DMS exp seq\n",
    "           'N':6,  #End token of DMS exp seq\n",
    "           'T':7,  #Start token of 2A3 exp seq\n",
    "           'X':8,  #End token of 2A3 exp seq\n",
    "           'Z':0   #Padding token\n",
    "          }\n",
    "#for s in df.at[0,'sequence']:\n",
    "#    print(seq_map[s])\n",
    "#df\n",
    "\n",
    "#test_cell = [seq_map[s] for s in df.at[0, 'sequence']]\n",
    "#print(test_cell)\n",
    "\n",
    "#len(df.at[0, 'sequence'])\n",
    "# Create a list of random data samples (for demonstration)\n",
    "#max_seq_len = 206 #This will change to 457 in private set  # Set the maximum sequence length to 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57653080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 8522.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over rows and replace a specific value in all columns\n",
    "\n",
    "#seq = [self.seq_map[s] for s in seq]\n",
    "#for index, row in df.iterrows():\n",
    "# Create a list of random data samples (for demonstration)\n",
    "#max_seq_len = 206 #This will change to 457 in private set  # Set the maximum sequence length to 100\n",
    "if data_preprocessing_done == 0:\n",
    "    df_sq = copy.deepcopy(df['sequence']) #saving a backup of sequences.\n",
    "    #react_columns['maped_sequence'] = [' ']*len(react_columns)\n",
    "    #for index, row in tqdm(df.iterrows(), total=len(df.rows), desc=\"Processing rows\"):\n",
    "    for index, row in tqdm(react_columns.iterrows(), total=len(react_columns), desc=\"Processing rows\"):\n",
    "        if df.at[index, 'experiment_type'] == \"2A3_MaP\":\n",
    "            concat_seq = 'T' + df.at[index, 'sequence'] + 'X'\n",
    "        if df.at[index, 'experiment_type'] == \"DMS_MaP\":\n",
    "            concat_seq = 'M' + df.at[index, 'sequence'] + 'N'\n",
    "        padding = \"Z\"*(max_seq_len - 2 - len(df.at[index, 'sequence']))\n",
    "        concatenated_seq = concat_seq + padding\n",
    "        df.at[index, 'sequence'] = [seq_map[s] for s in concatenated_seq]\n",
    "        #react_columns.at[index, 'maped_sequence'] = [seq_map[s] for s in concatenated_seq]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb2e3e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(df.at[0, 'sequence'])\n",
    "#df\n",
    "#react_columns['maped_sequence'] = [' ']*len(react_columns)\n",
    "#react_columns.to_pickle(\"react_columns.pkl\")\n",
    "if data_preprocessing_done == 0:\n",
    "    react_columns['reactivity_0207'] = copy.deepcopy(react_columns['reactivity_0206'])\n",
    "    react_columns['reactivity_0208'] = copy.deepcopy(react_columns['reactivity_0206'])\n",
    "    react_columns['mapped_sequence'] = copy.deepcopy(df['sequence'])\n",
    "    #del react_columns['reactivity_0207']\n",
    "    #del react_columns['reactivity_0208']\n",
    "if save_pkl == 1:\n",
    "    react_columns.to_pickle(\"react_columns.pkl\")\n",
    "\n",
    "if load_pkl == 1:\n",
    "    react_columns = pd.read_pickle(\"react_columns.pkl\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df1b4076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reactivity_0001</th>\n",
       "      <th>reactivity_0002</th>\n",
       "      <th>reactivity_0003</th>\n",
       "      <th>reactivity_0004</th>\n",
       "      <th>reactivity_0005</th>\n",
       "      <th>reactivity_0006</th>\n",
       "      <th>reactivity_0007</th>\n",
       "      <th>reactivity_0008</th>\n",
       "      <th>reactivity_0009</th>\n",
       "      <th>reactivity_0010</th>\n",
       "      <th>...</th>\n",
       "      <th>reactivity_0200</th>\n",
       "      <th>reactivity_0201</th>\n",
       "      <th>reactivity_0202</th>\n",
       "      <th>reactivity_0203</th>\n",
       "      <th>reactivity_0204</th>\n",
       "      <th>reactivity_0205</th>\n",
       "      <th>reactivity_0206</th>\n",
       "      <th>reactivity_0207</th>\n",
       "      <th>reactivity_0208</th>\n",
       "      <th>mapped_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>[7, 3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1, 1, 1, 1, 2, 1, 4, 4, 3, 1, 4, 1, 4, 3, 3, 1, 4, 4, 4, 1, 2, 4, 2, 2, 3, 1, 3, 3, 1, 3, 1, 2, 3, 1, 1, 2, 4, 1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 3, 3, 3, 3, 1, 1, 1, 2, 4, 2, 4, 1, 2, 2, 2, 3, 4, 3, 3, 2, 3, 4, 2, 4, 2, 2, 3, 4, 4, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.106801</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>[7, 3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1, 1, 1, 1, 2, 2, 4, 4, 3, 1, 4, 1, 4, 3, 3, 1, 4, 4, 4, 1, 2, 4, 2, 2, 3, 1, 3, 3, 1, 3, 1, 2, 3, 1, 1, 2, 4, 1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 3, 3, 3, 3, 1, 1, 1, 2, 4, 2, 4, 1, 2, 2, 2, 3, 4, 3, 3, 2, 3, 4, 2, 4, 2, 2, 3, 4, 4, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>[7, 3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1, 1, 1, 1, 2, 4, 4, 4, 3, 1, 4, 1, 4, 3, 3, 1, 4, 4, 4, 1, 2, 4, 2, 2, 3, 1, 3, 3, 1, 3, 1, 2, 3, 1, 1, 2, 4, 1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 3, 3, 3, 3, 1, 1, 1, 2, 4, 2, 4, 1, 2, 2, 2, 3, 4, 3, 3, 2, 3, 4, 2, 4, 2, 2, 3, 4, 4, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>[7, 3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1, 1, 1, 1, 2, 3, 1, 4, 3, 1, 4, 1, 4, 3, 3, 1, 4, 4, 4, 1, 2, 4, 2, 2, 3, 1, 3, 3, 1, 3, 1, 2, 3, 1, 1, 2, 4, 1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 3, 3, 3, 3, 1, 1, 1, 2, 4, 2, 4, 1, 2, 2, 2, 3, 4, 3, 3, 2, 3, 4, 2, 4, 2, 2, 3, 4, 4, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>[5, 3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1, 1, 1, 1, 2, 2, 4, 4, 3, 1, 4, 1, 4, 3, 3, 1, 4, 4, 4, 1, 2, 4, 2, 2, 3, 1, 3, 3, 1, 3, 1, 2, 3, 1, 1, 2, 4, 1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 3, 3, 3, 3, 1, 1, 1, 2, 4, 2, 4, 1, 2, 2, 2, 3, 4, 3, 3, 2, 3, 4, 2, 4, 2, 2, 3, 4, 4, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>[5, 3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1, 1, 1, 1, 2, 4, 4, 4, 3, 1, 4, 1, 4, 3, 3, 1, 4, 4, 4, 1, 2, 4, 2, 2, 3, 1, 3, 3, 1, 3, 1, 2, 3, 1, 1, 2, 4, 1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 3, 3, 3, 3, 1, 1, 1, 2, 4, 2, 4, 1, 2, 2, 2, 3, 4, 3, 3, 2, 3, 4, 2, 4, 2, 2, 3, 4, 4, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.244432</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>0.244432</td>\n",
       "      <td>[5, 3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1, 1, 1, 1, 2, 3, 1, 4, 3, 1, 4, 1, 4, 3, 3, 1, 4, 4, 4, 1, 2, 4, 2, 2, 3, 1, 3, 3, 1, 3, 1, 2, 3, 1, 1, 2, 4, 1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 3, 3, 3, 3, 1, 1, 1, 2, 4, 2, 4, 1, 2, 2, 2, 3, 4, 3, 3, 2, 3, 4, 2, 4, 2, 2, 3, 4, 4, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.245485</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>[5, 3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1, 1, 1, 1, 2, 3, 2, 4, 3, 1, 4, 1, 4, 3, 3, 1, 4, 4, 4, 1, 2, 4, 2, 2, 3, 1, 3, 3, 1, 3, 1, 2, 3, 1, 1, 2, 4, 1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 3, 3, 3, 3, 1, 1, 1, 2, 4, 2, 4, 1, 2, 2, 2, 3, 4, 3, 3, 2, 3, 4, 2, 4, 2, 2, 3, 4, 4, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 209 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   reactivity_0001  reactivity_0002  reactivity_0003  reactivity_0004  \\\n",
       "0         0.104340         0.104340         0.104340         0.104340   \n",
       "1         0.106801         0.106801         0.106801         0.106801   \n",
       "2         0.098947         0.098947         0.098947         0.098947   \n",
       "3         0.105214         0.105214         0.105214         0.105214   \n",
       "4         0.281481         0.281481         0.281481         0.281481   \n",
       "5         0.228019         0.228019         0.228019         0.228019   \n",
       "6         0.244432         0.244432         0.244432         0.244432   \n",
       "7         0.245485         0.245485         0.245485         0.245485   \n",
       "\n",
       "   reactivity_0005  reactivity_0006  reactivity_0007  reactivity_0008  \\\n",
       "0         0.104340         0.104340         0.104340         0.104340   \n",
       "1         0.106801         0.106801         0.106801         0.106801   \n",
       "2         0.098947         0.098947         0.098947         0.098947   \n",
       "3         0.105214         0.105214         0.105214         0.105214   \n",
       "4         0.281481         0.281481         0.281481         0.281481   \n",
       "5         0.228019         0.228019         0.228019         0.228019   \n",
       "6         0.244432         0.244432         0.244432         0.244432   \n",
       "7         0.245485         0.245485         0.245485         0.245485   \n",
       "\n",
       "   reactivity_0009  reactivity_0010  ...  reactivity_0200  reactivity_0201  \\\n",
       "0         0.104340         0.104340  ...         0.104340         0.104340   \n",
       "1         0.106801         0.106801  ...         0.106801         0.106801   \n",
       "2         0.098947         0.098947  ...         0.098947         0.098947   \n",
       "3         0.105214         0.105214  ...         0.105214         0.105214   \n",
       "4         0.281481         0.281481  ...         0.281481         0.281481   \n",
       "5         0.228019         0.228019  ...         0.228019         0.228019   \n",
       "6         0.244432         0.244432  ...         0.244432         0.244432   \n",
       "7         0.245485         0.245485  ...         0.245485         0.245485   \n",
       "\n",
       "   reactivity_0202  reactivity_0203  reactivity_0204  reactivity_0205  \\\n",
       "0         0.104340         0.104340         0.104340         0.104340   \n",
       "1         0.106801         0.106801         0.106801         0.106801   \n",
       "2         0.098947         0.098947         0.098947         0.098947   \n",
       "3         0.105214         0.105214         0.105214         0.105214   \n",
       "4         0.281481         0.281481         0.281481         0.281481   \n",
       "5         0.228019         0.228019         0.228019         0.228019   \n",
       "6         0.244432         0.244432         0.244432         0.244432   \n",
       "7         0.245485         0.245485         0.245485         0.245485   \n",
       "\n",
       "   reactivity_0206  reactivity_0207  reactivity_0208  \\\n",
       "0         0.104340         0.104340         0.104340   \n",
       "1         0.106801         0.106801         0.106801   \n",
       "2         0.098947         0.098947         0.098947   \n",
       "3         0.105214         0.105214         0.105214   \n",
       "4         0.281481         0.281481         0.281481   \n",
       "5         0.228019         0.228019         0.228019   \n",
       "6         0.244432         0.244432         0.244432   \n",
       "7         0.245485         0.245485         0.245485   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     mapped_sequence  \n",
       "0  [7, 3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1, 1, 1, 1, 2, 1, 4, 4, 3, 1, 4, 1, 4, 3, 3, 1, 4, 4, 4, 1, 2, 4, 2, 2, 3, 1, 3, 3, 1, 3, 1, 2, 3, 1, 1, 2, 4, 1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 3, 3, 3, 3, 1, 1, 1, 2, 4, 2, 4, 1, 2, 2, 2, 3, 4, 3, 3, 2, 3, 4, 2, 4, 2, 2, 3, 4, 4, ...]  \n",
       "1  [7, 3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1, 1, 1, 1, 2, 2, 4, 4, 3, 1, 4, 1, 4, 3, 3, 1, 4, 4, 4, 1, 2, 4, 2, 2, 3, 1, 3, 3, 1, 3, 1, 2, 3, 1, 1, 2, 4, 1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 3, 3, 3, 3, 1, 1, 1, 2, 4, 2, 4, 1, 2, 2, 2, 3, 4, 3, 3, 2, 3, 4, 2, 4, 2, 2, 3, 4, 4, ...]  \n",
       "2  [7, 3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1, 1, 1, 1, 2, 4, 4, 4, 3, 1, 4, 1, 4, 3, 3, 1, 4, 4, 4, 1, 2, 4, 2, 2, 3, 1, 3, 3, 1, 3, 1, 2, 3, 1, 1, 2, 4, 1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 3, 3, 3, 3, 1, 1, 1, 2, 4, 2, 4, 1, 2, 2, 2, 3, 4, 3, 3, 2, 3, 4, 2, 4, 2, 2, 3, 4, 4, ...]  \n",
       "3  [7, 3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1, 1, 1, 1, 2, 3, 1, 4, 3, 1, 4, 1, 4, 3, 3, 1, 4, 4, 4, 1, 2, 4, 2, 2, 3, 1, 3, 3, 1, 3, 1, 2, 3, 1, 1, 2, 4, 1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 3, 3, 3, 3, 1, 1, 1, 2, 4, 2, 4, 1, 2, 2, 2, 3, 4, 3, 3, 2, 3, 4, 2, 4, 2, 2, 3, 4, 4, ...]  \n",
       "4  [5, 3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1, 1, 1, 1, 2, 2, 4, 4, 3, 1, 4, 1, 4, 3, 3, 1, 4, 4, 4, 1, 2, 4, 2, 2, 3, 1, 3, 3, 1, 3, 1, 2, 3, 1, 1, 2, 4, 1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 3, 3, 3, 3, 1, 1, 1, 2, 4, 2, 4, 1, 2, 2, 2, 3, 4, 3, 3, 2, 3, 4, 2, 4, 2, 2, 3, 4, 4, ...]  \n",
       "5  [5, 3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1, 1, 1, 1, 2, 4, 4, 4, 3, 1, 4, 1, 4, 3, 3, 1, 4, 4, 4, 1, 2, 4, 2, 2, 3, 1, 3, 3, 1, 3, 1, 2, 3, 1, 1, 2, 4, 1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 3, 3, 3, 3, 1, 1, 1, 2, 4, 2, 4, 1, 2, 2, 2, 3, 4, 3, 3, 2, 3, 4, 2, 4, 2, 2, 3, 4, 4, ...]  \n",
       "6  [5, 3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1, 1, 1, 1, 2, 3, 1, 4, 3, 1, 4, 1, 4, 3, 3, 1, 4, 4, 4, 1, 2, 4, 2, 2, 3, 1, 3, 3, 1, 3, 1, 2, 3, 1, 1, 2, 4, 1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 3, 3, 3, 3, 1, 1, 1, 2, 4, 2, 4, 1, 2, 2, 2, 3, 4, 3, 3, 2, 3, 4, 2, 4, 2, 2, 3, 4, 4, ...]  \n",
       "7  [5, 3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1, 1, 1, 1, 2, 3, 2, 4, 3, 1, 4, 1, 4, 3, 3, 1, 4, 4, 4, 1, 2, 4, 2, 2, 3, 1, 3, 3, 1, 3, 1, 2, 3, 1, 1, 2, 4, 1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 3, 3, 3, 3, 1, 1, 1, 2, 4, 2, 4, 1, 2, 2, 2, 3, 4, 3, 3, 2, 3, 4, 2, 4, 2, 2, 3, 4, 4, ...]  \n",
       "\n",
       "[8 rows x 209 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#react_columns.select_dtypes(exclude=\"NaN\")\n",
    "#df[~df['COLUMN1'].str.contains('TOTAL')]\n",
    "#react_columns.iloc[~react_columns[:,1].str.contains('NaN')]\n",
    "\n",
    "#react_columns.iloc[:,1]\n",
    "\n",
    "#data1 = [[1, 1, 2], [6, 4, 2], [4, 2, 1], [4, 2, 3]]\n",
    "\n",
    "#daf = pd.DataFrame(data1)\n",
    "#print(data1)\n",
    "#print(daf.mean())\n",
    "#react_columns['sequence'][150:170][160]\n",
    "react_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02d4f1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for your data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, max_seq_len):\n",
    "        self.data = data\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #src_sequence, trg_sequence = [0,1] #self.data[idx]\n",
    "        #src_sequence = self.data['reactivity_0001']\n",
    "        #trg_sequence = self.data['mapped_sequence']\n",
    "        #list_src_trg_seq = self.data.loc[idx].apply(lambda row: row.tolist(), axis=1)\n",
    "        #merged_values = ','.join(map(str, df.loc[row_index_to_merge][:-1]))\n",
    "        #list_src_trg_seq = ','.join(map(str, self.data.loc[row_index_to_merge][:-1]))\n",
    "        #src_sequence = list_src_trg_seq[0:-1]\n",
    "        #srg_sequence = list_src_trg_seq[-1]\n",
    "        \n",
    "        #trg_sequence = ','.join(map(str, self.data.loc[idx][:-1]))\n",
    "        #src_sequence = [','.join(map(str, self.data.loc[idx][-1]))]\n",
    "        \n",
    "        trg_sequence = torch.tensor(self.data.iloc[idx, :].tolist()[:-1])\n",
    "        src_sequence = torch.tensor(self.data.iloc[idx, :].tolist()[-1])\n",
    "        \n",
    "        #print(\"Size src seq \", src_sequence.size())\n",
    "        #print(\"Size trg seq \", trg_sequence.size())\n",
    "        \n",
    "\n",
    "        #trg_sequence = self.data.iloc[idx, :].totensor()[:-1]\n",
    "        #src_sequence = self.data.iloc[idx, :].totensor()[-1]\n",
    "        \n",
    "        # Pad sequences with zeros to match the length of the longest sequence in each batch\n",
    "        #max_len = max(len(src_sequence), len(trg_sequence), self.max_seq_len)\n",
    "        #src_sequence += [0] * (max_len - len(src_sequence))\n",
    "        #trg_sequence += [0] * (max_len - len(trg_sequence))\n",
    "        #print(\"Index \", idx, \" SRC--\", src_sequence, \" TRG--\", trg_sequence)\n",
    "        #print(\"SRC LEN \", len(src_sequence), \" TRG LEN \", len(trg_sequence))\n",
    "        return src_sequence, trg_sequence\n",
    "\n",
    "\n",
    "\n",
    "#data = []\n",
    "#for _ in range(5):\n",
    "#    src_sequence = [random.randint(1, 100) for _ in range(random.randint(5, max_seq_len))]\n",
    "#    trg_sequence = [random.randint(1, 100) for _ in range(random.randint(5, max_seq_len))]\n",
    "#    data.append((src_sequence, trg_sequence))\n",
    "\n",
    "#print(\"src\", len(src_sequence))\n",
    "#print(\"trg\", len(trg_sequence))\n",
    "\n",
    "#list_src_trg_seq = react_columns.apply(lambda row: row.tolist(), axis=1)\n",
    "\n",
    "    \n",
    "# Create a DataLoader for batching and shuffling\n",
    "batch_size = 1\n",
    "#batch_size = 3\n",
    "\n",
    "\n",
    "# Define the size of the training set\n",
    "dataset_size = len(react_columns)\n",
    "train_size = int(0.9 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "# Create indices for training and testing sets\n",
    "#indices = list(range(dataset_size))\n",
    "#train_indices, test_indices = random_split(indices, [train_size, test_size])\n",
    "#print(train_indices)\n",
    "#print(test_indices)\n",
    "custom_dataset = CustomDataset(react_columns, max_seq_len)\n",
    "train_indices, test_indices = random_split(custom_dataset, [train_size, test_size])\n",
    "train_dataloader = DataLoader(train_indices, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_indices, batch_size=batch_size)\n",
    "\n",
    "#train_dataloader = DataLoader(custom_dataset, batch_size=batch_size)\n",
    "#test_dataloader = DataLoader(custom_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "#data = DataLoader(custom_dataset)\n",
    "  \n",
    "# Example usage in the training loop (as previously shown)\n",
    "#for batch in dataloader:\n",
    "#    print(\"Inside DataLoader Func\")\n",
    "#    src, trg = batch\n",
    "#    print(\"SRC -\", src, src.size())\n",
    "#    print(\"TRG -\", trg, trg.size())\n",
    "    # ...\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c30c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "#print(device)\n",
    "src_vocab_size = 512\n",
    "#tgt_vocab_size = 5000\n",
    "tgt_vocab_size = 512\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 1024\n",
    "max_seq_length = max_seq_len\n",
    "dropout = 0.1\n",
    "\n",
    "\n",
    "\n",
    "src_vocab_size = 5000\n",
    "#tgt_vocab_size = 5000\n",
    "tgt_vocab_size = 206\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = max_seq_len\n",
    "dropout = 0.1\n",
    "\n",
    "# Generate random sample data\n",
    "#src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)\n",
    "#tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)\n",
    "\n",
    "transformer_model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
    "\n",
    "#transformer = nn.Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b506449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "def optimizer_to(optim, device):\n",
    "    for param in optim.state.values():\n",
    "        # Not sure there are any global tensors in the state dict\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)\n",
    "\n",
    "\n",
    "print(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9, weight_decay=0, amsgrad=False)\n",
    "\n",
    "\n",
    "#transformer.train()\n",
    "epochx_train =[]\n",
    "epochx_test =[]\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32d69b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef los_func(src_data, tgt_data, tgt_vocab_size):\\n    transformer_model.eval()\\n    output = transformer_model(src_data, tgt_data[:, :-1])\\n    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\\n    transformer_model.train()\\n    return loss\\n\\nfor epoch in range(3):\\n    for src_data, tgt_data in train_dataloader:          \\n        src_data, tgt_data = src_data.to(device), tgt_data.to(device,  dtype=torch.long)\\n        learn = Learner(data, transformer_model, loss_func=los_func(src_data, tgt_data,tgt_vocab_size)).to_fp16() \\n        learn.fit_one_cycle(32, lr_max=5e-4, wd=0.05, pct_start=0.02)\\n        torch.save(learn.model.state_dict(),\"Model_Trained.model\")\\n        gc.collect()\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def los_func(src_data, tgt_data, tgt_vocab_size):\n",
    "    transformer_model.eval()\n",
    "    output = transformer_model(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    transformer_model.train()\n",
    "    return loss\n",
    "\n",
    "for epoch in range(3):\n",
    "    for src_data, tgt_data in train_dataloader:          \n",
    "        src_data, tgt_data = src_data.to(device), tgt_data.to(device,  dtype=torch.long)\n",
    "        learn = Learner(data, transformer_model, loss_func=los_func(src_data, tgt_data,tgt_vocab_size)).to_fp16() \n",
    "        learn.fit_one_cycle(32, lr_max=5e-4, wd=0.05, pct_start=0.02)\n",
    "        torch.save(learn.model.state_dict(),\"Model_Trained.model\")\n",
    "        gc.collect()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a59e78d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRG - tensor([[2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 1.4960e+00, 9.8200e-01, 1.4500e-01, 0.0000e+00,\n",
      "         2.9300e-01, 7.3000e-02, 9.9000e-02, 5.9300e-01, 2.4200e-01, 4.7200e-01,\n",
      "         0.0000e+00, 4.8500e-01, 0.0000e+00, 0.0000e+00, 7.3000e-02, 0.0000e+00,\n",
      "         3.4000e-01, 0.0000e+00, 1.6900e-01, 0.0000e+00, 2.6600e-01, 1.3400e-01,\n",
      "         5.0800e-01, 1.2400e-01, 1.2000e-02, 2.0600e-01, 8.5000e-02, 2.8000e-01,\n",
      "         1.1000e-01, 0.0000e+00, 4.9500e-01, 6.6900e-01, 1.4500e-01, 1.4160e+00,\n",
      "         4.6000e+00, 0.0000e+00, 0.0000e+00, 2.6600e-01, 5.7000e-01, 4.1200e-01,\n",
      "         8.8500e-01, 1.0150e+00, 1.4640e+00, 1.5250e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.1100e-01, 1.9700e-01, 4.3800e-01, 0.0000e+00,\n",
      "         1.8200e-01, 4.2400e-01, 3.6000e-02, 1.8880e+00, 2.2330e+00, 2.1000e+00,\n",
      "         1.0000e-03, 2.4200e-01, 6.1000e-02, 2.6400e-01, 4.0400e-01, 4.7500e-01,\n",
      "         4.0000e-03, 4.0000e-03, 0.0000e+00, 3.9900e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 3.6000e-02, 5.6600e-01, 1.3700e-01, 3.6300e-01, 2.6400e+00,\n",
      "         3.0270e+00, 1.1270e+00, 5.3300e-01, 0.0000e+00, 0.0000e+00, 5.6900e-01,\n",
      "         0.0000e+00, 3.4000e-01, 3.2900e-01, 0.0000e+00, 1.8200e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.9500e-01, 2.5400e-01, 2.1800e-01, 1.2100e+00,\n",
      "         1.4810e+00, 1.0300e+00, 2.0700e+00, 5.2100e-01, 3.2700e-01, 1.5860e+00,\n",
      "         7.6300e-01, 2.1900e-01, 2.1800e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 2, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         1, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 1, Training Loss: 5.908605098724365\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 1, Validation Loss: 1.2568590641021729\n",
      "TRG - tensor([[0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.3290,\n",
      "         1.6070, 0.2350, 0.2010, 0.3230, 0.0200, 1.0680, 0.1860, 0.5000, 0.0000,\n",
      "         0.0440, 0.0150, 1.2880, 0.1340, 0.0000, 0.1290, 0.5090, 0.0000, 0.3230,\n",
      "         0.0080, 0.1480, 0.1670, 0.8470, 0.0950, 0.2280, 0.2150, 0.2250, 0.1180,\n",
      "         0.2250, 0.0000, 0.5290, 0.2350, 1.0820, 0.8330, 2.7330, 1.7810, 0.0410,\n",
      "         0.2490, 0.2740, 0.1810, 0.4580, 2.0300, 0.5240, 1.3810, 0.4520, 0.2600,\n",
      "         0.1980, 0.4650, 0.0090, 0.2440, 0.2070, 0.7740, 0.4360, 0.1270, 0.2450,\n",
      "         1.1560, 3.6870, 1.2830, 0.0810, 0.2240, 0.0000, 0.0000, 0.4210, 0.0000,\n",
      "         0.1710, 0.0540, 0.0630, 1.1800, 0.0000, 0.3920, 0.2740, 0.2910, 0.6020,\n",
      "         0.1590, 0.0000, 1.6500, 6.4130, 0.1180, 0.1070, 0.0000, 0.0000, 0.0000,\n",
      "         0.4160, 0.0000, 0.1760, 0.1580, 0.0680, 0.2090, 0.0230, 0.5940, 0.0780,\n",
      "         0.0880, 0.0390, 1.2290, 0.0000, 0.3720, 1.1850, 0.8230, 0.5190, 0.3310,\n",
      "         0.6680, 0.3330, 0.0000, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 1, Training Loss: 1.6080994606018066\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 1, Validation Loss: 1.0865938663482666\n",
      "TRG - tensor([[0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.0000,\n",
      "         1.0180, 0.3540, 0.5770, 0.2370, 0.0970, 0.0000, 0.0630, 0.1830, 0.1840,\n",
      "         0.0000, 0.0000, 0.0970, 0.0980, 0.0000, 0.1140, 0.0000, 0.0000, 0.0000,\n",
      "         0.0140, 0.0130, 0.0000, 0.2820, 0.1720, 0.1450, 0.0030, 0.4140, 0.0560,\n",
      "         0.1550, 0.0700, 0.1290, 0.0570, 0.5160, 1.7290, 0.6110, 2.6670, 0.1900,\n",
      "         0.2540, 0.0000, 0.0940, 0.5630, 0.0850, 0.0610, 0.6740, 0.4750, 0.0540,\n",
      "         0.0170, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0290, 0.0000, 0.1690,\n",
      "         0.1020, 0.2320, 0.2390, 0.0030, 0.0000, 0.0000, 0.0660, 0.0780, 0.0000,\n",
      "         0.0000, 0.0000, 0.0240, 0.0000, 0.0000, 0.0000, 0.0500, 0.0800, 0.3980,\n",
      "         0.6800, 0.2290, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0310, 0.0460, 0.1580, 0.0000, 0.0090, 0.0000, 0.0000, 0.0000, 0.0740,\n",
      "         0.1210, 0.0890, 0.1620, 0.2650, 0.4110, 0.9270, 2.5160, 0.7170, 0.0530,\n",
      "         0.1930, 0.9200, 0.0810, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 1.4470146894454956\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 1, Validation Loss: 2.5952563285827637\n",
      "TRG - tensor([[1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 0.0000e+00, 6.8300e-01, 1.8900e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.7000e-02, 0.0000e+00, 1.4800e-01, 2.8000e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 9.0000e-03, 0.0000e+00, 2.8000e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 7.8000e-02, 2.8400e-01, 3.6100e-01,\n",
      "         6.3100e-01, 0.0000e+00, 0.0000e+00, 7.6000e-02, 5.1500e-01, 1.6900e-01,\n",
      "         2.0000e-01, 1.2000e-02, 2.6300e-01, 4.7700e-01, 5.3100e-01, 2.2830e+00,\n",
      "         9.8100e-01, 2.8210e+00, 1.5500e-01, 0.0000e+00, 3.0000e-03, 2.0000e-02,\n",
      "         4.1400e-01, 2.6200e-01, 1.8600e-01, 1.1380e+00, 0.0000e+00, 8.5000e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.9300e-01, 0.0000e+00, 1.8000e-02, 4.4800e-01, 0.0000e+00, 3.0300e-01,\n",
      "         0.0000e+00, 6.3000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1000e-01, 0.0000e+00,\n",
      "         2.0000e-03, 2.8200e-01, 2.3100e-01, 2.8200e-01, 1.2300e-01, 0.0000e+00,\n",
      "         1.0500e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.3000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 1.1200e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.8000e-02,\n",
      "         4.9000e-02, 0.0000e+00, 0.0000e+00, 1.0000e-02, 9.0000e-03, 0.0000e+00,\n",
      "         3.1300e-01, 3.2900e-01, 8.2300e-01, 1.8130e+00, 4.8400e-01, 6.1100e-01,\n",
      "         5.2400e-01, 4.5300e-01, 5.6600e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 1, Training Loss: 1.2499241828918457\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 1, Validation Loss: 2.624662399291992\n",
      "TRG - tensor([[9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 1.5600e-01, 4.3500e-01, 1.3800e-01, 1.6000e-01,\n",
      "         1.9400e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.3000e-02, 2.7700e-01,\n",
      "         0.0000e+00, 0.0000e+00, 8.0000e-03, 2.6000e-02, 0.0000e+00, 2.7000e-02,\n",
      "         2.8000e-02, 1.0000e-03, 0.0000e+00, 0.0000e+00, 1.5800e-01, 1.9800e-01,\n",
      "         5.6800e-01, 1.7000e-02, 1.0000e-03, 3.5000e-02, 4.4500e-01, 0.0000e+00,\n",
      "         6.1000e-02, 3.6000e-02, 2.7400e-01, 1.9600e-01, 7.9400e-01, 2.1350e+00,\n",
      "         8.3000e-01, 2.3150e+00, 9.0000e-03, 4.8000e-02, 5.2000e-02, 1.8400e-01,\n",
      "         4.9500e-01, 1.9300e-01, 3.1400e-01, 7.8200e-01, 1.9000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6000e-02,\n",
      "         5.8000e-02, 0.0000e+00, 7.9000e-02, 3.0600e-01, 9.8000e-02, 1.9200e-01,\n",
      "         1.1000e-02, 0.0000e+00, 1.3000e-02, 4.0000e-03, 0.0000e+00, 3.5000e-02,\n",
      "         2.1000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e-02, 0.0000e+00,\n",
      "         8.2000e-02, 4.0900e-01, 3.6000e-01, 5.6600e-01, 9.2000e-02, 3.2000e-02,\n",
      "         3.6000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6000e-02, 4.0000e-03,\n",
      "         3.0000e-03, 3.4000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.0000e-03, 1.2000e-02, 6.0000e-03, 0.0000e+00, 0.0000e+00, 4.2000e-02,\n",
      "         9.9000e-02, 2.9800e-01, 9.2900e-01, 1.7880e+00, 1.0550e+00, 2.4100e-01,\n",
      "         2.6000e-01, 4.1600e-01, 1.0800e+00, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 1.3292162418365479\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 1, Validation Loss: 1.524221658706665\n",
      "TRG - tensor([[0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.0000,\n",
      "         0.3540, 0.0000, 0.0000, 0.0000, 0.0160, 0.0000, 0.0500, 0.0000, 0.4690,\n",
      "         0.0000, 0.0000, 0.0830, 0.0980, 0.0900, 0.0000, 0.0000, 0.0000, 0.0450,\n",
      "         0.0000, 0.0090, 0.1210, 0.2590, 0.0000, 0.0000, 0.0830, 0.1790, 0.0000,\n",
      "         0.0770, 0.1690, 0.0970, 0.0230, 0.6090, 1.8070, 1.1750, 1.7910, 0.0210,\n",
      "         0.0000, 0.0000, 0.0470, 0.6690, 0.2600, 0.1090, 1.0150, 0.0280, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0630, 0.0000, 0.0030, 0.0790, 0.0390, 0.2180,\n",
      "         0.0820, 0.2660, 0.1110, 0.0000, 0.0290, 0.0660, 0.0990, 0.0000, 0.0420,\n",
      "         0.0000, 0.1630, 0.0930, 0.1150, 0.0000, 0.0000, 0.0000, 0.5610, 0.5680,\n",
      "         0.1950, 0.0000, 0.1760, 0.5100, 0.2970, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1650, 0.4080, 0.0000, 0.0000, 0.0000, 0.0000, 0.0420, 0.0000,\n",
      "         0.0640, 0.0000, 0.0330, 0.1320, 0.3130, 1.0220, 2.5630, 1.1310, 0.2370,\n",
      "         0.3490, 1.0900, 0.9240, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 1, Training Loss: 0.8678213357925415\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 1, Validation Loss: 1.1057469844818115\n",
      "TRG - tensor([[0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 1.9990,\n",
      "         0.3580, 3.0690, 1.1530, 0.0000, 0.1590, 0.0000, 0.0000, 0.0000, 0.1190,\n",
      "         0.8700, 0.0000, 0.5170, 0.1390, 0.1390, 0.0000, 0.0000, 0.0400, 0.0000,\n",
      "         0.7550, 0.0000, 0.0000, 0.9140, 0.2390, 0.0000, 0.8350, 0.3580, 0.6360,\n",
      "         0.0000, 0.8350, 0.0000, 0.0000, 1.3520, 0.8860, 2.5450, 0.2650, 0.4580,\n",
      "         0.4770, 0.0400, 0.0400, 1.5580, 0.0000, 2.5840, 2.4210, 0.8550, 0.8550,\n",
      "         0.0000, 0.0000, 0.4370, 0.0000, 0.9140, 0.1190, 0.3580, 1.1130, 0.0800,\n",
      "         1.3520, 0.2720, 4.7140, 0.0000, 0.0400, 0.0000, 0.3180, 0.2510, 0.0400,\n",
      "         0.3580, 0.4380, 0.0000, 0.1990, 0.0990, 0.0000, 0.1780, 0.0000, 0.2390,\n",
      "         1.6710, 0.0000, 0.6980, 3.6210, 0.0000, 0.0000, 0.0750, 0.0000, 0.0000,\n",
      "         0.0000, 0.5170, 0.5570, 0.0000, 0.0000, 0.0000, 0.6760, 0.0000, 0.0000,\n",
      "         0.7560, 0.0000, 0.0000, 0.0000, 3.0240, 1.5520, 2.3080, 0.1990, 1.6310,\n",
      "         1.1140, 0.5970, 0.0000, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 3, 1, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 1, Training Loss: 2.1676816940307617\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 1, Validation Loss: 1.1353604793548584\n",
      "TRG - tensor([[2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 1.4960e+00, 9.8200e-01, 1.4500e-01, 0.0000e+00,\n",
      "         2.9300e-01, 7.3000e-02, 9.9000e-02, 5.9300e-01, 2.4200e-01, 4.7200e-01,\n",
      "         0.0000e+00, 4.8500e-01, 0.0000e+00, 0.0000e+00, 7.3000e-02, 0.0000e+00,\n",
      "         3.4000e-01, 0.0000e+00, 1.6900e-01, 0.0000e+00, 2.6600e-01, 1.3400e-01,\n",
      "         5.0800e-01, 1.2400e-01, 1.2000e-02, 2.0600e-01, 8.5000e-02, 2.8000e-01,\n",
      "         1.1000e-01, 0.0000e+00, 4.9500e-01, 6.6900e-01, 1.4500e-01, 1.4160e+00,\n",
      "         4.6000e+00, 0.0000e+00, 0.0000e+00, 2.6600e-01, 5.7000e-01, 4.1200e-01,\n",
      "         8.8500e-01, 1.0150e+00, 1.4640e+00, 1.5250e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.1100e-01, 1.9700e-01, 4.3800e-01, 0.0000e+00,\n",
      "         1.8200e-01, 4.2400e-01, 3.6000e-02, 1.8880e+00, 2.2330e+00, 2.1000e+00,\n",
      "         1.0000e-03, 2.4200e-01, 6.1000e-02, 2.6400e-01, 4.0400e-01, 4.7500e-01,\n",
      "         4.0000e-03, 4.0000e-03, 0.0000e+00, 3.9900e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 3.6000e-02, 5.6600e-01, 1.3700e-01, 3.6300e-01, 2.6400e+00,\n",
      "         3.0270e+00, 1.1270e+00, 5.3300e-01, 0.0000e+00, 0.0000e+00, 5.6900e-01,\n",
      "         0.0000e+00, 3.4000e-01, 3.2900e-01, 0.0000e+00, 1.8200e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.9500e-01, 2.5400e-01, 2.1800e-01, 1.2100e+00,\n",
      "         1.4810e+00, 1.0300e+00, 2.0700e+00, 5.2100e-01, 3.2700e-01, 1.5860e+00,\n",
      "         7.6300e-01, 2.1900e-01, 2.1800e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 2, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         1, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 1.7482781410217285\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 2, Validation Loss: 1.1308691501617432\n",
      "TRG - tensor([[0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.3290,\n",
      "         1.6070, 0.2350, 0.2010, 0.3230, 0.0200, 1.0680, 0.1860, 0.5000, 0.0000,\n",
      "         0.0440, 0.0150, 1.2880, 0.1340, 0.0000, 0.1290, 0.5090, 0.0000, 0.3230,\n",
      "         0.0080, 0.1480, 0.1670, 0.8470, 0.0950, 0.2280, 0.2150, 0.2250, 0.1180,\n",
      "         0.2250, 0.0000, 0.5290, 0.2350, 1.0820, 0.8330, 2.7330, 1.7810, 0.0410,\n",
      "         0.2490, 0.2740, 0.1810, 0.4580, 2.0300, 0.5240, 1.3810, 0.4520, 0.2600,\n",
      "         0.1980, 0.4650, 0.0090, 0.2440, 0.2070, 0.7740, 0.4360, 0.1270, 0.2450,\n",
      "         1.1560, 3.6870, 1.2830, 0.0810, 0.2240, 0.0000, 0.0000, 0.4210, 0.0000,\n",
      "         0.1710, 0.0540, 0.0630, 1.1800, 0.0000, 0.3920, 0.2740, 0.2910, 0.6020,\n",
      "         0.1590, 0.0000, 1.6500, 6.4130, 0.1180, 0.1070, 0.0000, 0.0000, 0.0000,\n",
      "         0.4160, 0.0000, 0.1760, 0.1580, 0.0680, 0.2090, 0.0230, 0.5940, 0.0780,\n",
      "         0.0880, 0.0390, 1.2290, 0.0000, 0.3720, 1.1850, 0.8230, 0.5190, 0.3310,\n",
      "         0.6680, 0.3330, 0.0000, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 2, Training Loss: 1.2593178749084473\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 2, Validation Loss: 1.0920531749725342\n",
      "TRG - tensor([[0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.0000,\n",
      "         1.0180, 0.3540, 0.5770, 0.2370, 0.0970, 0.0000, 0.0630, 0.1830, 0.1840,\n",
      "         0.0000, 0.0000, 0.0970, 0.0980, 0.0000, 0.1140, 0.0000, 0.0000, 0.0000,\n",
      "         0.0140, 0.0130, 0.0000, 0.2820, 0.1720, 0.1450, 0.0030, 0.4140, 0.0560,\n",
      "         0.1550, 0.0700, 0.1290, 0.0570, 0.5160, 1.7290, 0.6110, 2.6670, 0.1900,\n",
      "         0.2540, 0.0000, 0.0940, 0.5630, 0.0850, 0.0610, 0.6740, 0.4750, 0.0540,\n",
      "         0.0170, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0290, 0.0000, 0.1690,\n",
      "         0.1020, 0.2320, 0.2390, 0.0030, 0.0000, 0.0000, 0.0660, 0.0780, 0.0000,\n",
      "         0.0000, 0.0000, 0.0240, 0.0000, 0.0000, 0.0000, 0.0500, 0.0800, 0.3980,\n",
      "         0.6800, 0.2290, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0310, 0.0460, 0.1580, 0.0000, 0.0090, 0.0000, 0.0000, 0.0000, 0.0740,\n",
      "         0.1210, 0.0890, 0.1620, 0.2650, 0.4110, 0.9270, 2.5160, 0.7170, 0.0530,\n",
      "         0.1930, 0.9200, 0.0810, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 2, Training Loss: 1.995511770248413\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 2, Validation Loss: 1.022186517715454\n",
      "TRG - tensor([[1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 0.0000e+00, 6.8300e-01, 1.8900e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.7000e-02, 0.0000e+00, 1.4800e-01, 2.8000e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 9.0000e-03, 0.0000e+00, 2.8000e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 7.8000e-02, 2.8400e-01, 3.6100e-01,\n",
      "         6.3100e-01, 0.0000e+00, 0.0000e+00, 7.6000e-02, 5.1500e-01, 1.6900e-01,\n",
      "         2.0000e-01, 1.2000e-02, 2.6300e-01, 4.7700e-01, 5.3100e-01, 2.2830e+00,\n",
      "         9.8100e-01, 2.8210e+00, 1.5500e-01, 0.0000e+00, 3.0000e-03, 2.0000e-02,\n",
      "         4.1400e-01, 2.6200e-01, 1.8600e-01, 1.1380e+00, 0.0000e+00, 8.5000e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.9300e-01, 0.0000e+00, 1.8000e-02, 4.4800e-01, 0.0000e+00, 3.0300e-01,\n",
      "         0.0000e+00, 6.3000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1000e-01, 0.0000e+00,\n",
      "         2.0000e-03, 2.8200e-01, 2.3100e-01, 2.8200e-01, 1.2300e-01, 0.0000e+00,\n",
      "         1.0500e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.3000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 1.1200e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.8000e-02,\n",
      "         4.9000e-02, 0.0000e+00, 0.0000e+00, 1.0000e-02, 9.0000e-03, 0.0000e+00,\n",
      "         3.1300e-01, 3.2900e-01, 8.2300e-01, 1.8130e+00, 4.8400e-01, 6.1100e-01,\n",
      "         5.2400e-01, 4.5300e-01, 5.6600e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 1.5487492084503174\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 2, Validation Loss: 0.9954688549041748\n",
      "TRG - tensor([[9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 1.5600e-01, 4.3500e-01, 1.3800e-01, 1.6000e-01,\n",
      "         1.9400e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.3000e-02, 2.7700e-01,\n",
      "         0.0000e+00, 0.0000e+00, 8.0000e-03, 2.6000e-02, 0.0000e+00, 2.7000e-02,\n",
      "         2.8000e-02, 1.0000e-03, 0.0000e+00, 0.0000e+00, 1.5800e-01, 1.9800e-01,\n",
      "         5.6800e-01, 1.7000e-02, 1.0000e-03, 3.5000e-02, 4.4500e-01, 0.0000e+00,\n",
      "         6.1000e-02, 3.6000e-02, 2.7400e-01, 1.9600e-01, 7.9400e-01, 2.1350e+00,\n",
      "         8.3000e-01, 2.3150e+00, 9.0000e-03, 4.8000e-02, 5.2000e-02, 1.8400e-01,\n",
      "         4.9500e-01, 1.9300e-01, 3.1400e-01, 7.8200e-01, 1.9000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6000e-02,\n",
      "         5.8000e-02, 0.0000e+00, 7.9000e-02, 3.0600e-01, 9.8000e-02, 1.9200e-01,\n",
      "         1.1000e-02, 0.0000e+00, 1.3000e-02, 4.0000e-03, 0.0000e+00, 3.5000e-02,\n",
      "         2.1000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e-02, 0.0000e+00,\n",
      "         8.2000e-02, 4.0900e-01, 3.6000e-01, 5.6600e-01, 9.2000e-02, 3.2000e-02,\n",
      "         3.6000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6000e-02, 4.0000e-03,\n",
      "         3.0000e-03, 3.4000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.0000e-03, 1.2000e-02, 6.0000e-03, 0.0000e+00, 0.0000e+00, 4.2000e-02,\n",
      "         9.9000e-02, 2.9800e-01, 9.2900e-01, 1.7880e+00, 1.0550e+00, 2.4100e-01,\n",
      "         2.6000e-01, 4.1600e-01, 1.0800e+00, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 2, Training Loss: 0.9779340028762817\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 2, Validation Loss: 1.130393624305725\n",
      "TRG - tensor([[0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.0000,\n",
      "         0.3540, 0.0000, 0.0000, 0.0000, 0.0160, 0.0000, 0.0500, 0.0000, 0.4690,\n",
      "         0.0000, 0.0000, 0.0830, 0.0980, 0.0900, 0.0000, 0.0000, 0.0000, 0.0450,\n",
      "         0.0000, 0.0090, 0.1210, 0.2590, 0.0000, 0.0000, 0.0830, 0.1790, 0.0000,\n",
      "         0.0770, 0.1690, 0.0970, 0.0230, 0.6090, 1.8070, 1.1750, 1.7910, 0.0210,\n",
      "         0.0000, 0.0000, 0.0470, 0.6690, 0.2600, 0.1090, 1.0150, 0.0280, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0630, 0.0000, 0.0030, 0.0790, 0.0390, 0.2180,\n",
      "         0.0820, 0.2660, 0.1110, 0.0000, 0.0290, 0.0660, 0.0990, 0.0000, 0.0420,\n",
      "         0.0000, 0.1630, 0.0930, 0.1150, 0.0000, 0.0000, 0.0000, 0.5610, 0.5680,\n",
      "         0.1950, 0.0000, 0.1760, 0.5100, 0.2970, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1650, 0.4080, 0.0000, 0.0000, 0.0000, 0.0000, 0.0420, 0.0000,\n",
      "         0.0640, 0.0000, 0.0330, 0.1320, 0.3130, 1.0220, 2.5630, 1.1310, 0.2370,\n",
      "         0.3490, 1.0900, 0.9240, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.6193047761917114\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 2, Validation Loss: 1.3473241329193115\n",
      "TRG - tensor([[0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 1.9990,\n",
      "         0.3580, 3.0690, 1.1530, 0.0000, 0.1590, 0.0000, 0.0000, 0.0000, 0.1190,\n",
      "         0.8700, 0.0000, 0.5170, 0.1390, 0.1390, 0.0000, 0.0000, 0.0400, 0.0000,\n",
      "         0.7550, 0.0000, 0.0000, 0.9140, 0.2390, 0.0000, 0.8350, 0.3580, 0.6360,\n",
      "         0.0000, 0.8350, 0.0000, 0.0000, 1.3520, 0.8860, 2.5450, 0.2650, 0.4580,\n",
      "         0.4770, 0.0400, 0.0400, 1.5580, 0.0000, 2.5840, 2.4210, 0.8550, 0.8550,\n",
      "         0.0000, 0.0000, 0.4370, 0.0000, 0.9140, 0.1190, 0.3580, 1.1130, 0.0800,\n",
      "         1.3520, 0.2720, 4.7140, 0.0000, 0.0400, 0.0000, 0.3180, 0.2510, 0.0400,\n",
      "         0.3580, 0.4380, 0.0000, 0.1990, 0.0990, 0.0000, 0.1780, 0.0000, 0.2390,\n",
      "         1.6710, 0.0000, 0.6980, 3.6210, 0.0000, 0.0000, 0.0750, 0.0000, 0.0000,\n",
      "         0.0000, 0.5170, 0.5570, 0.0000, 0.0000, 0.0000, 0.6760, 0.0000, 0.0000,\n",
      "         0.7560, 0.0000, 0.0000, 0.0000, 3.0240, 1.5520, 2.3080, 0.1990, 1.6310,\n",
      "         1.1140, 0.5970, 0.0000, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 3, 1, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 2, Training Loss: 1.6125136613845825\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 2, Validation Loss: 1.5224664211273193\n",
      "TRG - tensor([[2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 1.4960e+00, 9.8200e-01, 1.4500e-01, 0.0000e+00,\n",
      "         2.9300e-01, 7.3000e-02, 9.9000e-02, 5.9300e-01, 2.4200e-01, 4.7200e-01,\n",
      "         0.0000e+00, 4.8500e-01, 0.0000e+00, 0.0000e+00, 7.3000e-02, 0.0000e+00,\n",
      "         3.4000e-01, 0.0000e+00, 1.6900e-01, 0.0000e+00, 2.6600e-01, 1.3400e-01,\n",
      "         5.0800e-01, 1.2400e-01, 1.2000e-02, 2.0600e-01, 8.5000e-02, 2.8000e-01,\n",
      "         1.1000e-01, 0.0000e+00, 4.9500e-01, 6.6900e-01, 1.4500e-01, 1.4160e+00,\n",
      "         4.6000e+00, 0.0000e+00, 0.0000e+00, 2.6600e-01, 5.7000e-01, 4.1200e-01,\n",
      "         8.8500e-01, 1.0150e+00, 1.4640e+00, 1.5250e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.1100e-01, 1.9700e-01, 4.3800e-01, 0.0000e+00,\n",
      "         1.8200e-01, 4.2400e-01, 3.6000e-02, 1.8880e+00, 2.2330e+00, 2.1000e+00,\n",
      "         1.0000e-03, 2.4200e-01, 6.1000e-02, 2.6400e-01, 4.0400e-01, 4.7500e-01,\n",
      "         4.0000e-03, 4.0000e-03, 0.0000e+00, 3.9900e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 3.6000e-02, 5.6600e-01, 1.3700e-01, 3.6300e-01, 2.6400e+00,\n",
      "         3.0270e+00, 1.1270e+00, 5.3300e-01, 0.0000e+00, 0.0000e+00, 5.6900e-01,\n",
      "         0.0000e+00, 3.4000e-01, 3.2900e-01, 0.0000e+00, 1.8200e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.9500e-01, 2.5400e-01, 2.1800e-01, 1.2100e+00,\n",
      "         1.4810e+00, 1.0300e+00, 2.0700e+00, 5.2100e-01, 3.2700e-01, 1.5860e+00,\n",
      "         7.6300e-01, 2.1900e-01, 2.1800e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01,\n",
      "         2.4443e-01, 2.4443e-01, 2.4443e-01, 2.4443e-01]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 2, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         1, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 1.3905000686645508\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 3, Validation Loss: 1.5518141984939575\n",
      "TRG - tensor([[0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.3290,\n",
      "         1.6070, 0.2350, 0.2010, 0.3230, 0.0200, 1.0680, 0.1860, 0.5000, 0.0000,\n",
      "         0.0440, 0.0150, 1.2880, 0.1340, 0.0000, 0.1290, 0.5090, 0.0000, 0.3230,\n",
      "         0.0080, 0.1480, 0.1670, 0.8470, 0.0950, 0.2280, 0.2150, 0.2250, 0.1180,\n",
      "         0.2250, 0.0000, 0.5290, 0.2350, 1.0820, 0.8330, 2.7330, 1.7810, 0.0410,\n",
      "         0.2490, 0.2740, 0.1810, 0.4580, 2.0300, 0.5240, 1.3810, 0.4520, 0.2600,\n",
      "         0.1980, 0.4650, 0.0090, 0.2440, 0.2070, 0.7740, 0.4360, 0.1270, 0.2450,\n",
      "         1.1560, 3.6870, 1.2830, 0.0810, 0.2240, 0.0000, 0.0000, 0.4210, 0.0000,\n",
      "         0.1710, 0.0540, 0.0630, 1.1800, 0.0000, 0.3920, 0.2740, 0.2910, 0.6020,\n",
      "         0.1590, 0.0000, 1.6500, 6.4130, 0.1180, 0.1070, 0.0000, 0.0000, 0.0000,\n",
      "         0.4160, 0.0000, 0.1760, 0.1580, 0.0680, 0.2090, 0.0230, 0.5940, 0.0780,\n",
      "         0.0880, 0.0390, 1.2290, 0.0000, 0.3720, 1.1850, 0.8230, 0.5190, 0.3310,\n",
      "         0.6680, 0.3330, 0.0000, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455, 0.2455,\n",
      "         0.2455]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 3, Training Loss: 1.4346539974212646\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 3, Validation Loss: 1.4111484289169312\n",
      "TRG - tensor([[0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.0000,\n",
      "         1.0180, 0.3540, 0.5770, 0.2370, 0.0970, 0.0000, 0.0630, 0.1830, 0.1840,\n",
      "         0.0000, 0.0000, 0.0970, 0.0980, 0.0000, 0.1140, 0.0000, 0.0000, 0.0000,\n",
      "         0.0140, 0.0130, 0.0000, 0.2820, 0.1720, 0.1450, 0.0030, 0.4140, 0.0560,\n",
      "         0.1550, 0.0700, 0.1290, 0.0570, 0.5160, 1.7290, 0.6110, 2.6670, 0.1900,\n",
      "         0.2540, 0.0000, 0.0940, 0.5630, 0.0850, 0.0610, 0.6740, 0.4750, 0.0540,\n",
      "         0.0170, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0290, 0.0000, 0.1690,\n",
      "         0.1020, 0.2320, 0.2390, 0.0030, 0.0000, 0.0000, 0.0660, 0.0780, 0.0000,\n",
      "         0.0000, 0.0000, 0.0240, 0.0000, 0.0000, 0.0000, 0.0500, 0.0800, 0.3980,\n",
      "         0.6800, 0.2290, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0310, 0.0460, 0.1580, 0.0000, 0.0090, 0.0000, 0.0000, 0.0000, 0.0740,\n",
      "         0.1210, 0.0890, 0.1620, 0.2650, 0.4110, 0.9270, 2.5160, 0.7170, 0.0530,\n",
      "         0.1930, 0.9200, 0.0810, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052, 0.1052,\n",
      "         0.1052]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 3, Training Loss: 0.8334500193595886\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 3, Validation Loss: 1.2923719882965088\n",
      "TRG - tensor([[1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 0.0000e+00, 6.8300e-01, 1.8900e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.7000e-02, 0.0000e+00, 1.4800e-01, 2.8000e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 9.0000e-03, 0.0000e+00, 2.8000e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 7.8000e-02, 2.8400e-01, 3.6100e-01,\n",
      "         6.3100e-01, 0.0000e+00, 0.0000e+00, 7.6000e-02, 5.1500e-01, 1.6900e-01,\n",
      "         2.0000e-01, 1.2000e-02, 2.6300e-01, 4.7700e-01, 5.3100e-01, 2.2830e+00,\n",
      "         9.8100e-01, 2.8210e+00, 1.5500e-01, 0.0000e+00, 3.0000e-03, 2.0000e-02,\n",
      "         4.1400e-01, 2.6200e-01, 1.8600e-01, 1.1380e+00, 0.0000e+00, 8.5000e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.9300e-01, 0.0000e+00, 1.8000e-02, 4.4800e-01, 0.0000e+00, 3.0300e-01,\n",
      "         0.0000e+00, 6.3000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1000e-01, 0.0000e+00,\n",
      "         2.0000e-03, 2.8200e-01, 2.3100e-01, 2.8200e-01, 1.2300e-01, 0.0000e+00,\n",
      "         1.0500e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.3000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 1.1200e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.8000e-02,\n",
      "         4.9000e-02, 0.0000e+00, 0.0000e+00, 1.0000e-02, 9.0000e-03, 0.0000e+00,\n",
      "         3.1300e-01, 3.2900e-01, 8.2300e-01, 1.8130e+00, 4.8400e-01, 6.1100e-01,\n",
      "         5.2400e-01, 4.5300e-01, 5.6600e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01,\n",
      "         1.0434e-01, 1.0434e-01, 1.0434e-01, 1.0434e-01]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.7752211093902588\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 3, Validation Loss: 1.2106611728668213\n",
      "TRG - tensor([[9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 1.5600e-01, 4.3500e-01, 1.3800e-01, 1.6000e-01,\n",
      "         1.9400e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.3000e-02, 2.7700e-01,\n",
      "         0.0000e+00, 0.0000e+00, 8.0000e-03, 2.6000e-02, 0.0000e+00, 2.7000e-02,\n",
      "         2.8000e-02, 1.0000e-03, 0.0000e+00, 0.0000e+00, 1.5800e-01, 1.9800e-01,\n",
      "         5.6800e-01, 1.7000e-02, 1.0000e-03, 3.5000e-02, 4.4500e-01, 0.0000e+00,\n",
      "         6.1000e-02, 3.6000e-02, 2.7400e-01, 1.9600e-01, 7.9400e-01, 2.1350e+00,\n",
      "         8.3000e-01, 2.3150e+00, 9.0000e-03, 4.8000e-02, 5.2000e-02, 1.8400e-01,\n",
      "         4.9500e-01, 1.9300e-01, 3.1400e-01, 7.8200e-01, 1.9000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6000e-02,\n",
      "         5.8000e-02, 0.0000e+00, 7.9000e-02, 3.0600e-01, 9.8000e-02, 1.9200e-01,\n",
      "         1.1000e-02, 0.0000e+00, 1.3000e-02, 4.0000e-03, 0.0000e+00, 3.5000e-02,\n",
      "         2.1000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e-02, 0.0000e+00,\n",
      "         8.2000e-02, 4.0900e-01, 3.6000e-01, 5.6600e-01, 9.2000e-02, 3.2000e-02,\n",
      "         3.6000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6000e-02, 4.0000e-03,\n",
      "         3.0000e-03, 3.4000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.0000e-03, 1.2000e-02, 6.0000e-03, 0.0000e+00, 0.0000e+00, 4.2000e-02,\n",
      "         9.9000e-02, 2.9800e-01, 9.2900e-01, 1.7880e+00, 1.0550e+00, 2.4100e-01,\n",
      "         2.6000e-01, 4.1600e-01, 1.0800e+00, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02,\n",
      "         9.8947e-02, 9.8947e-02, 9.8947e-02, 9.8947e-02]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 3, Training Loss: 0.781302273273468\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 3, Validation Loss: 1.1510941982269287\n",
      "TRG - tensor([[0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.0000,\n",
      "         0.3540, 0.0000, 0.0000, 0.0000, 0.0160, 0.0000, 0.0500, 0.0000, 0.4690,\n",
      "         0.0000, 0.0000, 0.0830, 0.0980, 0.0900, 0.0000, 0.0000, 0.0000, 0.0450,\n",
      "         0.0000, 0.0090, 0.1210, 0.2590, 0.0000, 0.0000, 0.0830, 0.1790, 0.0000,\n",
      "         0.0770, 0.1690, 0.0970, 0.0230, 0.6090, 1.8070, 1.1750, 1.7910, 0.0210,\n",
      "         0.0000, 0.0000, 0.0470, 0.6690, 0.2600, 0.1090, 1.0150, 0.0280, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0630, 0.0000, 0.0030, 0.0790, 0.0390, 0.2180,\n",
      "         0.0820, 0.2660, 0.1110, 0.0000, 0.0290, 0.0660, 0.0990, 0.0000, 0.0420,\n",
      "         0.0000, 0.1630, 0.0930, 0.1150, 0.0000, 0.0000, 0.0000, 0.5610, 0.5680,\n",
      "         0.1950, 0.0000, 0.1760, 0.5100, 0.2970, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1650, 0.4080, 0.0000, 0.0000, 0.0000, 0.0000, 0.0420, 0.0000,\n",
      "         0.0640, 0.0000, 0.0330, 0.1320, 0.3130, 1.0220, 2.5630, 1.1310, 0.2370,\n",
      "         0.3490, 1.0900, 0.9240, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068,\n",
      "         0.1068]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.6773747205734253\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 3, Validation Loss: 1.0746290683746338\n",
      "TRG - tensor([[0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 1.9990,\n",
      "         0.3580, 3.0690, 1.1530, 0.0000, 0.1590, 0.0000, 0.0000, 0.0000, 0.1190,\n",
      "         0.8700, 0.0000, 0.5170, 0.1390, 0.1390, 0.0000, 0.0000, 0.0400, 0.0000,\n",
      "         0.7550, 0.0000, 0.0000, 0.9140, 0.2390, 0.0000, 0.8350, 0.3580, 0.6360,\n",
      "         0.0000, 0.8350, 0.0000, 0.0000, 1.3520, 0.8860, 2.5450, 0.2650, 0.4580,\n",
      "         0.4770, 0.0400, 0.0400, 1.5580, 0.0000, 2.5840, 2.4210, 0.8550, 0.8550,\n",
      "         0.0000, 0.0000, 0.4370, 0.0000, 0.9140, 0.1190, 0.3580, 1.1130, 0.0800,\n",
      "         1.3520, 0.2720, 4.7140, 0.0000, 0.0400, 0.0000, 0.3180, 0.2510, 0.0400,\n",
      "         0.3580, 0.4380, 0.0000, 0.1990, 0.0990, 0.0000, 0.1780, 0.0000, 0.2390,\n",
      "         1.6710, 0.0000, 0.6980, 3.6210, 0.0000, 0.0000, 0.0750, 0.0000, 0.0000,\n",
      "         0.0000, 0.5170, 0.5570, 0.0000, 0.0000, 0.0000, 0.6760, 0.0000, 0.0000,\n",
      "         0.7560, 0.0000, 0.0000, 0.0000, 3.0240, 1.5520, 2.3080, 0.1990, 1.6310,\n",
      "         1.1140, 0.5970, 0.0000, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815,\n",
      "         0.2815]], dtype=torch.float64) torch.Size([1, 208])\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 3, 1, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 3, Training Loss: 1.4672635793685913\n",
      "Src_embedded size torch.Size([1, 208, 512])\n",
      "tgt size  torch.Size([1, 207])\n",
      "TGT  =  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt_embedded size torch.Size([1, 207, 512])\n",
      "Epoch: 3, Validation Loss: 1.0301203727722168\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer_model.train()\n",
    "epochx_train =[]\n",
    "epochx_test =[]\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(3):\n",
    "    for src_data, tgt_data in train_dataloader:\n",
    "        #    print(\"Inside DataLoader Func\")\n",
    "        #src_data, trg_data = src_data.to(device), trg_data.to(device)\n",
    "        #print(\"SRC -\", src_data, src_data.size())\n",
    "        print(\"TRG -\", tgt_data, tgt_data.size())\n",
    "        #src_data = torch.tensor(src_data)\n",
    "        #trg_data = torch.tensor(trg_data)\n",
    "        src_data, tgt_data = src_data.to(device), tgt_data.to(device,  dtype=torch.long)\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        transformer_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        #src_data = src_data.to(device)\n",
    "        #x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "        #y = y.to(device=device, dtype=torch.long)\n",
    "        #tgt_data = tgt_data.to(device)\n",
    "        #print(\"TRG DATA \", tgt_data.size())\n",
    "        output = transformer_model(src_data, tgt_data[:, :-1])\n",
    "        #output = transformer(src_data, tgt_data)\n",
    "        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch: {epoch+1}, Training Loss: {loss.item()}\")\n",
    "        train_loss += loss.item()\n",
    "        train_losses.append(train_loss / len(src_data))\n",
    "        epochx_train.append(epoch+1)\n",
    "        #validation part\n",
    "        #Restructure code with validation set data\n",
    "        transformer_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for src_data, tgt_data in test_dataloader:\n",
    "                #This should be validation set data.\n",
    "                src_data = src_data.to(device=device)\n",
    "                tgt_data = tgt_data.to(device=device, dtype=torch.long)\n",
    "                output = transformer_model(src_data, tgt_data[:, :-1])\n",
    "                loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "                print(f\"Epoch: {epoch+1}, Validation Loss: {loss.item()}\")\n",
    "                val_loss += loss.item()\n",
    "                val_losses.append(val_loss / len(src_data))\n",
    "                epochx_test.append(epoch+1)\n",
    "        \n",
    "\n",
    "torch.save(transformer_model, \"Trained_Model.model\")\n",
    "\n",
    "#print(epochx_train, len(epochx_train))\n",
    "#print(train_losses, len(train_losses))\n",
    "# Plot both training and validation losses\n",
    "#plt.bar(epochx_train, height=0)\n",
    "plt.plot(epochx_train, train_losses, label='Training Loss')\n",
    "plt.plot(epochx_test, val_losses,  label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a0751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53570b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29d418c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation predict\n",
    "#val_df = \n",
    "\n",
    "#Read TEST Sequence\n",
    "test_df = pd.read_csv('/media/spartans/COMMON:/RNA_Project/test_sequences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1416e68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GGGAACGACUCGAGUAGAGUCGAAAAUUUCCUUCCAAAUCCUGAGGGAGAGAUAGAGGCGGAGGGUCUGGGGGAGGAAUUAAAACACAAGGUCUCCUCCCCUCUCGCCUGUCCGAACUUGGGGGCACCCCGGCUCGUACUUCGGUACGAGCCGGGGAAAAGAAACAACAACAACAAC'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(4)\n",
    "test_df.at[0, 'sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d90ca43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df['sequence'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa5461d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  48%|███████████████████████████████████████████████████████████                                                                | 645957/1343823 [01:14<01:15, 9283.20it/s]"
     ]
    }
   ],
   "source": [
    "test_df['DMS_sequence'] = copy.deepcopy(test_df['sequence'])\n",
    "test_df['2A3_sequence'] = copy.deepcopy(test_df['sequence'])\n",
    "\n",
    "\n",
    "test_df_sq = copy.deepcopy(test_df['sequence']) #saving a backup of sequences.\n",
    "#react_columns['maped_sequence'] = [' ']*len(react_columns)\n",
    "#for index, row in tqdm(df.iterrows(), total=len(df.rows), desc=\"Processing rows\"):\n",
    "for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing rows\"):\n",
    "    \n",
    "    padding = \"Z\"*(max_seq_len - 2 - len(test_df.at[index, 'sequence']))\n",
    "    form_DMS_seq = 'M' + test_df.at[index, 'sequence'] + 'N' + padding\n",
    "    form_2A3_seq = 'T' + test_df.at[index, 'sequence'] + 'X' + padding\n",
    "    \n",
    "\n",
    "    test_df.at[index, 'DMS_sequence'] = [seq_map[s] for s in form_DMS_seq]\n",
    "    test_df.at[index, '2A3_sequence'] = [seq_map[s] for s in form_2A3_seq]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.no_grad():\n",
    "#src_data = src_data.to(device=device)\n",
    "#tgt_data = tgt_data.to(device=device, dtype=torch.long)\n",
    "#output = transformer_model(src_data, tgt_data[:, :-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
