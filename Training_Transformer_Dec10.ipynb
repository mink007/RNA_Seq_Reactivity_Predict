{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10dd24c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformer import Transformer \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "#import polars as pl\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "import fastai\n",
    "from fastai.vision.all import *\n",
    "\n",
    "from fastai.data.load import DataLoader\n",
    "from fastai.learner import Learner\n",
    "from torch.utils.data import Dataset\n",
    "from fastai.metrics import rmse\n",
    "from fastai.losses import *\n",
    "\n",
    "import math\n",
    "import copy\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#device = \"cpu\"\n",
    "\n",
    "\n",
    "max_seq_len = 179 #103 #510 + 2  # +2 is added to cover start and end token of sequence This will change to 457 in private set  # Set the maximum sequence length to 100\n",
    "\n",
    "data_preprocessing_done = 0\n",
    "save_pkl = 0\n",
    "load_pkl = 0\n",
    "small_dataset = 0\n",
    "\n",
    "#Training Data\n",
    "if small_dataset == 1:\n",
    "    \n",
    "    #Uncomment this section when uploading the jupyter notebook\n",
    "    '''\n",
    "    df_read = pd.read_csv('/media/spartans/COMMON:/RNA_Project/train_data.csv')\n",
    "    \n",
    "    criteria_2A3 = (df_read['experiment_type'] == '2A3_MaP') & (df_read['SN_filter'] == 1)\n",
    "    criteria_DMS = (df_read['experiment_type'] == 'DMS_MaP') & (df_read['SN_filter'] == 1)\n",
    "\n",
    "    df_2A3_Filter1 = df_read[criteria_2A3]\n",
    "    df_2A3_Filter1 = df_2A3_Filter1.head(4)\n",
    "\n",
    "    df_DMS_Filter1 = df_read[criteria_DMS]\n",
    "    df_DMS_Filter1 = df_DMS_Filter1.head(4)\n",
    "\n",
    "   \n",
    "    df = pd.concat([df_2A3_Filter1, df_DMS_Filter1], ignore_index=True)\n",
    "    \n",
    "    df.to_csv('2A3_DMS_With_Filter1_Train_Data.csv', index=False)\n",
    "    #df_2A3_Filter1.to_csv('2A3_DMS_With_Filter1_Train_Data.csv', index=False)\n",
    "    '''\n",
    "    df = pd.read_csv('2A3_DMS_With_Filter1_Train_Data.csv')\n",
    "    #df = pd.read_csv('DMS_Data.csv')\n",
    "    #df = pd.read_csv('2A3_Data.csv')\n",
    "else:\n",
    "    df = pd.read_csv('/media/spartans/COMMON:/RNA_Project/train_data.csv')\n",
    "    \n",
    "\n",
    "#test_df = pd.read_csv('/media/spartans/COMMON:/RNA_Project/test_sequences.csv')\n",
    "TEST_PATH = \"/media/spartans/COMMON:/RNA_Project/test_sequences.csv\"\n",
    "SUBMISSION_PATH = \"/media/spartans/COMMON:/RNA_Project/submission_1.csv\"\n",
    "SAMPLE_SUBMISSION_PATH = \"/media/spartans/COMMON:/RNA_Project/sample_submission.csv\"\n",
    "MODEL_NAME = \"Trained_Model.model_512_12_10_2023\"\n",
    "\n",
    "seed=69\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = str(1)\n",
    "\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = str(1)\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2deb452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#criteria_2A3 = (df_read['experiment_type'] == '2A3_MaP') & (df_read['SN_filter'] == '1')\n",
    "#criteria_DMS = (df_read['experiment_type'] == 'DMS_MaP') & (df_read['SN_filter'] == '1')\n",
    "\n",
    "#criteria_2A3 = ((df_read['experiment_type'] == 'DMS_MaP')  & df_read['SN_filter'] == 1 )\n",
    "#df_2A3_Filter1 = df_read[criteria_2A3]\n",
    "#df_2A3_Filter1\n",
    "#df\n",
    "#df.iloc[2,0:300].values\n",
    "# Set display options to show all columns and rows\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "166c7518",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        #print(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4cee06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward1(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "    \n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        #print(f\"x after first linear layer: {x.size()}\")\n",
    "        x = self.relu(x)\n",
    "        #print(f\"x after activation: {x.size()}\")\n",
    "        x = self.dropout(x)\n",
    "        #print(f\"x after dropout: {x.size()}\")\n",
    "        x = self.linear2(x)\n",
    "        #print(f\"x after 2nd linear layer: {x.size()}\")\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c2bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        #print(self.pe[:, :x.size(1)].size())\n",
    "        #print((x + self.pe[:, :x.size(1)]).size())\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "    \n",
    "\n",
    "class PositionalEncoding1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        print(\"PE size\", PE.size())\n",
    "        print(\"x size\", x.size())\n",
    "        print(\"PE x size\", PE[:, :x.size(1)].size())\n",
    "        print(\"stacked dim\", stacked.size())\n",
    "        return x + PE[:, :x.size(1)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aff55ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d0f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b1354f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        \n",
    "        #self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_model, 179)\n",
    "\n",
    "\n",
    "        #self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        #self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        \n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=d_ff,\n",
    "                dropout=0.1, activation=nn.GELU(), batch_first=True, norm_first=True), num_layers)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #self.transformer_decoder = nn.TransformerDecoder(\n",
    "        #    nn.TransformerDecoderLayer(d_model=d_model, nhead=num_heads),\n",
    "        #    num_layers=num_layers\n",
    "        #)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc1 = nn.Linear(max_seq_length,max_seq_length)\n",
    "        \n",
    "        # Linear layer for regression output\n",
    "        #self.regressor = nn.Linear(d_model, max_seq_length)\n",
    "        self.regressor = nn.Linear(d_model, max_seq_length, dtype=torch.float32)  # Adjusted for float32\n",
    "\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(device)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        #print(\"forward \",src, src.size())\n",
    "        #print(\"forward --\", tgt, tgt.size())\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        #print(\"src mask --\", src_mask, src_mask.size(), tgt_mask, tgt_mask.size())\n",
    "        \n",
    "        \n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        #print(\"Src_embedded size\", src_embedded, src_embedded.size())\n",
    "        #print(\"tgt size \", tgt.size())\n",
    "        #print(\"TGT  = \", tgt)\n",
    "        #tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "        #print(\"tgt_embedded size\", tgt_embedded.size())\n",
    "\n",
    "        #enc_output = src_embedded\n",
    "        #for enc_layer in self.encoder_layers:\n",
    "        #    enc_output = enc_layer(enc_output, src_mask)\n",
    "            \n",
    "        enc_output = self.transformer(src_embedded)\n",
    "\n",
    "        #dec_output = tgt_embedded\n",
    "        #for dec_layer in self.decoder_layers:\n",
    "        #    dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        \n",
    "        ################\n",
    "        # Global average pooling along the sequence dimension\n",
    "        tgt = enc_output.mean(dim=1)\n",
    "\n",
    "        # Apply GELU activation to the output\n",
    "        tgt = nn.functional.gelu(tgt)\n",
    "\n",
    "        # Regression output with sigmoid activation to clip between 0 and 1\n",
    "        final_out = torch.sigmoid(self.regressor(tgt))\n",
    "        ################\n",
    "        \n",
    "        \n",
    "        # Expand the dimensions to create an initial input for the decoder\n",
    "        #x = enc_output.unsqueeze(1).expand(-1, self.output_sequence_length, -1)\n",
    "\n",
    "\n",
    "        #output = self.fc(dec_output)\n",
    "        #print(\"Size OUTPUT\",output.size())\n",
    "        \n",
    "        #xx = self.gelu(self.fc(dec_output))\n",
    "        #print(\"Size xx\", xx.size())\n",
    "        #final_out = self.fc1(xx)\n",
    "        #print(\"Size final_out\", final_out, final_out.size())\n",
    "        return final_out\n",
    "        #include here FC layers with output dimension = input dimension of the sequence (considering padding this will be const number)\n",
    "        #return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e6eb5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef7b613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#src_data\n",
    "#src_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddfe523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#src_data.size()\n",
    "#src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4f5478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61508b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#where reactivity is NaN,\n",
    "# have Nan replaced with average of all numbers in the sequence.\n",
    "\n",
    "#Map sequence letters to numbers.\n",
    "\n",
    "#Looking at Reads and Signal To Noise ..they appear to be somewhat coorelated.\n",
    "\n",
    "#Create Dataset function\n",
    "\n",
    "\n",
    "#In contrastive loss model... you can train based on#\n",
    "#in a sequence which part is 2D fold and which part is 3D fold\n",
    "#See notebook https://www.kaggle.com/code/something4kag/ribonanza-3d-coords-prep\n",
    "#if it can be helpful\n",
    "# Also analyze and which position in the sequence the fold can occur and which fold ended up happening based on the data\n",
    "\n",
    "#another method to formulate contrastive learning model is by deferentiating sequences with low SNR and hence reactivity value will not be with high confidence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db2f1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#In RNA, the most common base pairings you'll find are between the following nucleotide bases:\n",
    "\n",
    "#Adenine (A) and Uracil (U): A forms base pairs with U. This is a fundamental pairing and is commonly seen in RNA molecules, particularly in single-stranded regions. It is important for the stability of stem-loop structures and is a key component of RNA secondary structure.\n",
    "\n",
    "#Guanine (G) and Cytosine (C): G forms base pairs with C, just as it does in DNA. This pairing is less common in RNA secondary structure but is still important for certain RNA molecules. It may be more prevalent in the context of ribozymes and catalytic RNA.\n",
    "\n",
    "#While A-U and G-C are the primary base pairings in RNA, it's essential to understand that RNA can also exhibit non-canonical or non-standard base pairings, especially in more complex RNA structures. These non-canonical pairings can involve different combinations of A, U, G, and C and are often seen in tertiary structures or specialized RNA molecules with specific functions.\n",
    "\n",
    "#The prevalence of A-U and G-C base pairings in an RNA molecule can vary depending on its sequence and function. For instance, regions that need to form stable secondary structures often rely on A-U pairings, while regions involved in catalytic activities may include G-C pairs. RNA structures are diverse and can exhibit a wide range of base pairing interactions to achieve their biological functions.\n",
    "\n",
    "\n",
    "\n",
    "#So a contrastive loss can be formed here where\n",
    "#AU pair GC pair are strong positive - stable\n",
    "#AG, UC are less stable\n",
    "#UG, UA, CG pair are unstable so negative .\n",
    "#\n",
    "#In RNA, the stable base pairs among the four nucleotide bases (A, G, U, and C) follow the standard Watson-Crick base-pairing rules. Here are the stable base pairs among these bases:\n",
    "\n",
    "#GC pairs are more stable than AU pair\n",
    "\n",
    "\n",
    "#So you can tokenzie these pairs in the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5b71bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formulate Convolution NN or Dense Net which takes SNR, Reactivity error and feeds into the final layer of Transformer network\n",
    "# As an example\n",
    "#Lowest Reactivity Error and High SNR are  strong positives\n",
    "# Lowest SNR are strong negative.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#sequence length range from 115 to 206 in train dataset.\n",
    "#in final it will range from 207 to 457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dbb5976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In positional encoding\n",
    "#pos means the position of the word in the sequence.\n",
    "#i means the index for that particular word vector (index of the dimension)\n",
    "#dmodel is the dimension for e.g 512 from the example\n",
    "\n",
    "\n",
    "#For 2 experiment types i.e. DMS and 2A3 you can have start and end  token in a sequence for which data is available.\n",
    "#.e. start_DMS, end_DMS, start_2A3, end_2A3.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd9a8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6045989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.iloc[:,150:213]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8607779c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb051b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_154709/1617856248.py:18: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  react_columns = react_columns.applymap(lambda x: -1*x if isinstance(x, (float, int)) and x < 0 else x)\n"
     ]
    }
   ],
   "source": [
    "if data_preprocessing_done == 0:\n",
    "    #Assigning reactivity Columns to react_columns dataframe and replacing NaN with 0.000 \n",
    "\n",
    "    #del(mean_reactivity)\n",
    "    #del(react_columns_New)\n",
    "    #del(react_columns)\n",
    "    \n",
    "    #react_columns = df.iloc[:,7:213].fillna(0.000000)\n",
    "    \n",
    "    react_columns = df.iloc[:,33:136].fillna(0.000000)\n",
    "\n",
    "\n",
    "    #Find mean of reactivity per row\n",
    "    #df['mean'] = df.mean(axis=1)\n",
    "    #react_columns = react_columns.applymap(lambda x: 0.0 if x < 0.0 else x)\n",
    "    #react_columns = react_columns.applymap(lambda x: 0 if isinstance(x, (float, int)) and x < 0 else x)\n",
    "    \n",
    "    react_columns = react_columns.applymap(lambda x: -1*x if isinstance(x, (float, int)) and x < 0 else x)\n",
    "    react_columns = react_columns.applymap(lambda x: 1.0 if isinstance(x, (float, int)) and x > 1.0 else x)\n",
    "\n",
    "    #react_columns_mean = react_columns.mean(axis=1)\n",
    "    #Free up memory\n",
    "    #del(react_columns)\n",
    "\n",
    "    #Now replace NaN in react_columns with mean_reactivity\n",
    "    #react_columns_New = df.iloc[:,7:213].fillna(mean_reactivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6bd190",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#if data_preprocessing_done == 0:\n",
    "#    react_columns = df.iloc[:,7:213] #You may have to capture those indices as per sequence length i.e. when it extends to 457\n",
    "\n",
    "    # Replace NaN values in each row with values from replace_values array\n",
    "#    for index, row in tqdm(react_columns.iterrows(), total=len(react_columns), desc=\"Processing rows\"):\n",
    "#        react_columns.loc[index] = row.fillna(react_columns_mean[index])\n",
    "    \n",
    "#    react_columns = react_columns.applymap(lambda x: 0 if isinstance(x, (float, int)) and x < 0 else x)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95559ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#react_columns_New = df.iloc[:,7:213].fillna(react_columns['mean_reactivity'])\n",
    "#react_columns_New\n",
    "#react_columns.columns\n",
    "\n",
    "\n",
    "#for column_name in tqdm(react_columns.columns, total=len(react_columns.columns), desc=\"Processing\"):\n",
    "#    df[column_name] = react_columns[column_name]\n",
    "    \n",
    "#react_columns['sequence'] = df['experiment_type'] + 'start' + df['sequence']\n",
    "#react_columns['sequence'] = react_columns['sequence'] + 'end' + df['experiment_type']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_map1 = {'A':1.0,\n",
    "           'C':2.0,\n",
    "           'G':3.0,\n",
    "           'U':4.0, \n",
    "           'M':5.0,  #Start token of DMS exp seq\n",
    "           'N':6.0,  #End token of DMS exp seq\n",
    "           'T':7.0,  #Start token of 2A3 exp seq\n",
    "           'X':8.0,  #End token of 2A3 exp seq\n",
    "           'Z':0.0   #Padding token\n",
    "          }\n",
    "\n",
    "\n",
    "seq_map = {'A':1,\n",
    "           'C':2,\n",
    "           'G':3,\n",
    "           'U':4, \n",
    "           'M':5,  #Start token of DMS exp seq\n",
    "           'N':6,  #End token of DMS exp seq\n",
    "           'T':7,  #Start token of 2A3 exp seq\n",
    "           'X':8,  #End token of 2A3 exp seq\n",
    "           'Z':0   #Padding token\n",
    "          }\n",
    "#for s in df.at[0,'sequence']:\n",
    "#    print(seq_map[s])\n",
    "#df\n",
    "\n",
    "#test_cell = [seq_map[s] for s in df.at[0, 'sequence']]\n",
    "#print(test_cell)\n",
    "\n",
    "#len(df.at[0, 'sequence'])\n",
    "# Create a list of random data samples (for demonstration)\n",
    "#max_seq_len = 206 #This will change to 457 in private set  # Set the maximum sequence length to 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57653080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over rows and replace a specific value in all columns\n",
    "\n",
    "#seq = [self.seq_map[s] for s in seq]\n",
    "#for index, row in df.iterrows():\n",
    "# Create a list of random data samples (for demonstration)\n",
    "#max_seq_len = 206 #This will change to 457 in private set  # Set the maximum sequence length to 100\n",
    "if data_preprocessing_done == 0:\n",
    "    df_sq = copy.deepcopy(df['sequence']) #saving a backup of sequences.\n",
    "    #react_columns['maped_sequence'] = [' ']*len(react_columns)\n",
    "    #for index, row in tqdm(df.iterrows(), total=len(df.rows), desc=\"Processing rows\"):\n",
    "    for index, row in tqdm(react_columns.iterrows(), total=len(react_columns), desc=\"Processing rows\"):\n",
    "        if df.at[index, 'experiment_type'] == \"2A3_MaP\":\n",
    "            concat_seq = 'T' + df.at[index, 'sequence'] + 'X'\n",
    "        if df.at[index, 'experiment_type'] == \"DMS_MaP\":\n",
    "            concat_seq = 'M' + df.at[index, 'sequence'] + 'N'\n",
    "        padding = \"Z\"*(max_seq_len - 2 - len(df.at[index, 'sequence']))\n",
    "        concatenated_seq = concat_seq + padding\n",
    "        df.at[index, 'sequence'] = [seq_map[s] for s in concatenated_seq]\n",
    "        #react_columns.at[index, 'maped_sequence'] = [seq_map[s] for s in concatenated_seq]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2e3e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "if data_preprocessing_done == 0:\n",
    "    #react_columns['reactivity_0207'] = copy.deepcopy(react_columns['reactivity_0206'])\n",
    "    #react_columns['reactivity_0208'] = copy.deepcopy(react_columns['reactivity_0206'])\n",
    "    \n",
    "    for iii in range(130,206):\n",
    "        react_col_name = \"reactivity_0\" + str(iii)\n",
    "        react_columns[react_col_name] = copy.deepcopy(react_columns['reactivity_0129'])\n",
    "        \n",
    "    #replicated_columns = [react_columns['reactivity_0206'].repeat(2)]\n",
    "    #react_columns = pd.concat([react_columns, replicated_columns] , axis=1, ignore_index=True)\n",
    "    #print(react_columns)\n",
    "    react_columns['mapped_sequence'] = copy.deepcopy(df['sequence'])\n",
    "    #del react_columns['reactivity_0207']\n",
    "    #del react_columns['reactivity_0208']\n",
    "if save_pkl == 1:\n",
    "    react_columns.to_pickle(\"react_columns.pkl\")\n",
    "\n",
    "if load_pkl == 1:\n",
    "    react_columns = pd.read_pickle(\"react_columns.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b4076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#react_columns.select_dtypes(exclude=\"NaN\")\n",
    "#df[~df['COLUMN1'].str.contains('TOTAL')]\n",
    "#react_columns.iloc[~react_columns[:,1].str.contains('NaN')]\n",
    "\n",
    "#react_columns.iloc[:,1]\n",
    "\n",
    "#data1 = [[1, 1, 2], [6, 4, 2], [4, 2, 1], [4, 2, 3]]\n",
    "\n",
    "#daf = pd.DataFrame(data1)\n",
    "#print(data1)\n",
    "#print(daf.mean())\n",
    "#react_columns['sequence'][150:170][160]\n",
    "react_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d4f1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for your data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, max_seq_len):\n",
    "        self.data = data\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #src_sequence, trg_sequence = [0,1] #self.data[idx]\n",
    "        #src_sequence = self.data['reactivity_0001']\n",
    "        #trg_sequence = self.data['mapped_sequence']\n",
    "        #list_src_trg_seq = self.data.loc[idx].apply(lambda row: row.tolist(), axis=1)\n",
    "        #merged_values = ','.join(map(str, df.loc[row_index_to_merge][:-1]))\n",
    "        #list_src_trg_seq = ','.join(map(str, self.data.loc[row_index_to_merge][:-1]))\n",
    "        #src_sequence = list_src_trg_seq[0:-1]\n",
    "        #srg_sequence = list_src_trg_seq[-1]\n",
    "        \n",
    "        #trg_sequence = ','.join(map(str, self.data.loc[idx][:-1]))\n",
    "        #src_sequence = [','.join(map(str, self.data.loc[idx][-1]))]\n",
    "        \n",
    "        trg_sequence = torch.tensor(self.data.iloc[idx, :].tolist()[:-1])\n",
    "        src_sequence = torch.tensor(self.data.iloc[idx, :].tolist()[-1])\n",
    "        \n",
    "        #print(\"Size src seq \", src_sequence.size())\n",
    "        #print(\"Size trg seq \", trg_sequence, trg_sequence.size())\n",
    "        \n",
    "\n",
    "        #trg_sequence = self.data.iloc[idx, :].totensor()[:-1]\n",
    "        #src_sequence = self.data.iloc[idx, :].totensor()[-1]\n",
    "        \n",
    "        # Pad sequences with zeros to match the length of the longest sequence in each batch\n",
    "        #max_len = max(len(src_sequence), len(trg_sequence), self.max_seq_len)\n",
    "        #src_sequence += [0] * (max_len - len(src_sequence))\n",
    "        #trg_sequence += [0] * (max_len - len(trg_sequence))\n",
    "        #print(\"Index \", idx, \" SRC--\", src_sequence, \" TRG--\", trg_sequence)\n",
    "        #print(\"SRC LEN \", len(src_sequence), \" TRG LEN \", len(trg_sequence))\n",
    "        return src_sequence, trg_sequence\n",
    "\n",
    "\n",
    "\n",
    "#data = []\n",
    "#for _ in range(5):\n",
    "#    src_sequence = [random.randint(1, 100) for _ in range(random.randint(5, max_seq_len))]\n",
    "#    trg_sequence = [random.randint(1, 100) for _ in range(random.randint(5, max_seq_len))]\n",
    "#    data.append((src_sequence, trg_sequence))\n",
    "\n",
    "#print(\"src\", len(src_sequence))\n",
    "#print(\"trg\", len(trg_sequence))\n",
    "\n",
    "#list_src_trg_seq = react_columns.apply(lambda row: row.tolist(), axis=1)\n",
    "\n",
    "    \n",
    "# Create a DataLoader for batching and shuffling\n",
    "batch_size = 1\n",
    "#batch_size = 3\n",
    "\n",
    "\n",
    "# Define the size of the training set\n",
    "dataset_size = len(react_columns)\n",
    "train_size = int(0.9 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "# Create indices for training and testing sets\n",
    "#indices = list(range(dataset_size))\n",
    "#train_indices, test_indices = random_split(indices, [train_size, test_size])\n",
    "#print(train_indices)\n",
    "#print(test_indices)\n",
    "custom_dataset = CustomDataset(react_columns, max_seq_len)\n",
    "train_indices, test_indices = random_split(custom_dataset, [train_size, test_size])\n",
    "train_dataloader = DataLoader(train_indices, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_indices, batch_size=batch_size)\n",
    "\n",
    "#train_dataloader = DataLoader(custom_dataset, batch_size=batch_size)\n",
    "#test_dataloader = DataLoader(custom_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "#data = DataLoader(custom_dataset)\n",
    "  \n",
    "# Example usage in the training loop (as previously shown)\n",
    "#for batch in dataloader:\n",
    "#    print(\"Inside DataLoader Func\")\n",
    "#    src, trg = batch\n",
    "#    print(\"SRC -\", src, src.size())\n",
    "#    print(\"TRG -\", trg, trg.size())\n",
    "    # ...\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "#print(device)\n",
    "src_vocab_size = 512\n",
    "#tgt_vocab_size = 5000\n",
    "tgt_vocab_size = 512\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 1024\n",
    "max_seq_length = max_seq_len\n",
    "dropout = 0.1\n",
    "\n",
    "\n",
    "\n",
    "src_vocab_size = 5000\n",
    "#tgt_vocab_size = 5000\n",
    "tgt_vocab_size = 103\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = max_seq_len\n",
    "dropout = 0.1\n",
    "\n",
    "# Generate random sample data\n",
    "#src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)\n",
    "#tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)\n",
    "\n",
    "transformer_model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
    "\n",
    "#transformer = nn.Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d69b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def los_func(src_data, tgt_data, tgt_vocab_size):\n",
    "    transformer_model.eval()\n",
    "    output = transformer_model(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    transformer_model.train()\n",
    "    return loss\n",
    "\n",
    "for epoch in range(3):\n",
    "    for src_data, tgt_data in train_dataloader:          \n",
    "        src_data, tgt_data = src_data.to(device), tgt_data.to(device,  dtype=torch.long)\n",
    "        learn = Learner(data, transformer_model, loss_func=los_func(src_data, tgt_data,tgt_vocab_size)).to_fp16() \n",
    "        learn.fit_one_cycle(32, lr_max=5e-4, wd=0.05, pct_start=0.02)\n",
    "        torch.save(learn.model.state_dict(),\"Model_Trained.model\")\n",
    "        gc.collect()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e78d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "#criterion = nn.MSELoss()\n",
    "\n",
    "#criterion = nn.L1Loss(size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "\n",
    "\n",
    "def criterion(pred, target):\n",
    "    loss = F.l1_loss(pred, target.clip(0,1), reduction='none')\n",
    "    loss = loss[~torch.isnan(loss)].mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_losses = []  # to store training losses\n",
    "validation_losses = []\n",
    "prev_average_val_loss = 10000000000\n",
    "list_of_avg_loss = []\n",
    "epochss = []\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0  # accumulate loss for the entire epoch\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    transformer_model.train()\n",
    "    #for src_data, tgt_data in train_dataloader:\n",
    "    print(\"IN TRAIN\")\n",
    "    for batch, dat in enumerate(tqdm(train_dataloader, desc=\"Processing batches\", unit=\"batch\"), 1):\n",
    "\n",
    "        #print(\"TARGET DATA - TRAINING \", tgt_data, tgt_data.size())\n",
    "        \n",
    "        src_data, tgt_data = dat[0].to(device, dtype=torch.long), dat[1].to(device, dtype=torch.float32)\n",
    "        \n",
    "        #print(\"TARGET DATA - TRAINING \", tgt_data, tgt_data.size())\n",
    "        #print(\"SRC DATA - TRAINING \",  src_data, src_data.size())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = transformer_model(src_data, tgt_data)\n",
    "        #print(\"OUPUT DATA - TRAINING \",  output, output.size())\n",
    "        #loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "        loss = criterion(output, tgt_data)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "          \n",
    "        epoch_loss += loss.item()\n",
    "            \n",
    "    \n",
    "    transformer_model.eval()\n",
    "    with torch.no_grad():\n",
    "            #for src_data, tgt_data in test_dataloader:\n",
    "            print(\"IN VAL\")\n",
    "            for batch, dat in enumerate(tqdm(test_dataloader, desc=\"Processing batches\", unit=\"batch\"), 1):\n",
    "                #This should be validation set data.\n",
    "                src_data, tgt_data = dat[0].to(device, dtype=torch.long), dat[1].to(device, dtype=torch.float32)\n",
    "                #src_data, tgt_data = src_data.to(device, dtype=torch.long), tgt_data.to(device, dtype=torch.float32)\n",
    "                #print(\"src \", src_data)\n",
    "                #print(\"src size\", src_data.size())\n",
    "                #print(\"tgt \", tgt_data)\n",
    "                #print(\"tgt size\", tgt_data.size())\n",
    "                #output = transformer_model(src_data, tgt_data[:, :-1])\n",
    "                output = transformer_model(src_data, tgt_data)\n",
    "                #print(\"Target Data - VALIDATION \", tgt_data, tgt_data.size())\n",
    "                #print(\"Predicted Target Data - Validation \" , output, output.size())\n",
    "                \n",
    "                \n",
    "                #output[output < 0] = 0.0\n",
    "                #output[output > 1] = 1.0\n",
    "                loss = criterion(output, tgt_data)\n",
    "                #print(f\"Epoch: {epoch+1}, Validation Loss: {loss.item()}\")\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "\n",
    "    #torch.save(transformer_model, \"Trained_Model.model\")\n",
    "\n",
    "    average_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    training_losses.append(average_epoch_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {average_epoch_loss:.4f}')\n",
    "    \n",
    "    average_val_loss = val_loss / len(test_dataloader)\n",
    "    validation_losses.append(average_val_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {average_val_loss:.4f}')\n",
    "    epochss.append(epoch)\n",
    "    \n",
    "    list_of_avg_loss.append(average_val_loss)\n",
    "    #Save best model Every Epoch\n",
    "    if average_val_loss < prev_average_val_loss:\n",
    "        torch.save(transformer_model.state_dict(), MODEL_NAME)\n",
    "        \n",
    "        print(\"Model Saved because \")\n",
    "        print(f'Current average validation loss {average_val_loss:.4f} is less than previous average validation loss {prev_average_val_loss:.4f}')\n",
    "    prev_average_val_loss = min(list_of_avg_loss)\n",
    "\n",
    "# Plotting the training loss\n",
    "plt.plot(epochss, training_losses, label='Training Loss')\n",
    "plt.plot(epochss, validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a0751",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53570b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.load_state_dict(torch.load(MODEL_NAME,map_location=torch.device(device)))\n",
    "transformer_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d418c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Read TEST Sequence\n",
    "test_df = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f6c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df['DMS_sequence'] = copy.deepcopy(test_df['sequence'])\n",
    "test_df['2A3_sequence'] = copy.deepcopy(test_df['sequence'])\n",
    "\n",
    "\n",
    "test_df_sq = copy.deepcopy(test_df['sequence']) #saving a backup of sequences.\n",
    "#react_columns['maped_sequence'] = [' ']*len(react_columns)\n",
    "#for index, row in tqdm(df.iterrows(), total=len(df.rows), desc=\"Processing rows\"):\n",
    "for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing rows\"):\n",
    "    \n",
    "    padding = \"Z\"*(max_seq_len - 2 - len(test_df.at[index, 'sequence']))\n",
    "    form_DMS_seq = 'M' + test_df.at[index, 'sequence'] + 'N' + padding\n",
    "    form_2A3_seq = 'T' + test_df.at[index, 'sequence'] + 'X' + padding\n",
    "    \n",
    "\n",
    "    test_df.at[index, 'DMS_sequence'] = [seq_map[s] for s in form_DMS_seq]\n",
    "    test_df.at[index, '2A3_sequence'] = [seq_map[s] for s in form_2A3_seq]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c738e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tgt_sequence.size())\n",
    "\n",
    "#test_df['id_max'][0]\n",
    "#print(test_df)\n",
    "sample_df['reactivity_DMS_MaP'] = sample_df['reactivity_DMS_MaP'].astype(float)\n",
    "sample_df['reactivity_2A3_MaP'] = sample_df['reactivity_DMS_MaP'].astype(float)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d31b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_sequence = torch.rand((1, 179), dtype=torch.float32).to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482b60c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(\"seq split\", seq_split)\n",
    "element_to_prepend_2a3 = torch.tensor([7]).to(device=device, dtype=torch.long)\n",
    "element_to_postpend_2a3 = torch.tensor([8]).to(device=device, dtype=torch.long)\n",
    "\n",
    "element_to_prepend_dms = torch.tensor([5]).to(device=device, dtype=torch.long)\n",
    "element_to_postpend_dms = torch.tensor([6]).to(device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing rows\"):\n",
    "    input_sequence = torch.tensor([test_df.at[index, 'DMS_sequence']]).to(device=device, dtype=torch.long)\n",
    "    #print(input_sequence, input_sequence.shape)\n",
    "    seq_length = len(test_df.at[index, 'sequence'])\n",
    "    #print(f'{seq_length}')\n",
    "    #if seq_length != 177:\n",
    "        #print(f' seq length {seq_len}')\n",
    "        #id_minn = test_df.at[index, 'id_min']\n",
    "        #id_maxx = test_df.at[index, 'id_max']\n",
    "        #print(f'index {index}, seq len {seq_length}, id_min {id_minn},  id_max {id_maxx} ')\n",
    "    #First check what is the sequence length. If it is less than or equal to 206 then process it straight.\n",
    "    \n",
    "    id_minn = test_df.at[index, 'id_min']\n",
    "    id_maxx = test_df.at[index, 'id_max']\n",
    "    #print(f'index {index}, seq len {seq_length}, id_min {id_minn},  id_max {id_maxx} , {input_sequence.size()} {tgt_sequence.size()}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    seq_split = seq_length/179\n",
    "\n",
    "\n",
    "    if seq_split > 1 and seq_split < 2:\n",
    "        in_sq_0 = input_sequence[0][1:178] \n",
    "        in_sq_0_2a3 = torch.cat((element_to_prepend_2a3, in_sq_0, element_to_postpend_2a3))\n",
    "        out_2a3_0 = transformer_model(in_sq_0_2a3,tgt_sequence).to(dtype=torch.float16)\n",
    "\n",
    "        in_sq_0_dms = torch.cat((element_to_prepend_dms, in_sq_0, element_to_postpend_dms))\n",
    "        out_dms_0 = transformer_model(in_sq_0_dms,tgt_sequence).to(dtype=torch.float16)\n",
    "\n",
    "\n",
    "\n",
    "        in_sq_1 = input_sequence[0][179:-1]\n",
    "\n",
    "        in_sq_1_2a3 = torch.cat((element_to_prepend_2a3, in_sq_1, element_to_postpend_2a3))\n",
    "\n",
    "\n",
    "\n",
    "        padding = (179 - len(in_sq_1_2a3))\n",
    "        zeros_to_append = torch.zeros(padding).to(device=device, dtype=torch.long)\n",
    "        in_sq_1_2a3 = torch.cat((in_sq_1_2a3, zeros_to_append))\n",
    "\n",
    "        out_2a3_1 = transformer_model(in_sq_1_2a3,tgt_sequence).to(dtype=torch.float16)\n",
    "\n",
    "        in_sq_1_dms = torch.cat((element_to_prepend_dms, in_sq_1, element_to_postpend_dms))    \n",
    "        in_sq_1_dms = torch.cat((in_sq_1_dms, zeros_to_append))\n",
    "        out_dms_1 = transformer_model(in_sq_1_dms,tgt_sequence).to(dtype=torch.float16)\n",
    "\n",
    "        iiii = 0\n",
    "        for iii in range(id_minn, id_minn + 178):\n",
    "            sample_df.at[iii, 'reactivity_2A3_MaP'] = out_2a3_0.tolist()[0][iiii]  \n",
    "            sample_df.at[iii, 'reactivity_DMS_MaP'] = out_dms_0.tolist()[0][iiii]\n",
    "            iiii = iiii + 1  \n",
    "\n",
    "        iiii = 0\n",
    "        for iii in range(id_minn + 179, id_maxx):\n",
    "            sample_df.at[iii, 'reactivity_2A3_MaP'] = out_2a3_1.tolist()[0][iiii]  \n",
    "            sample_df.at[iii, 'reactivity_DMS_MaP'] = out_dms_1.tolist()[0][iiii]\n",
    "            iiii = iiii + 1 \n",
    "\n",
    "        #print(\"1 here\")\n",
    "\n",
    "\n",
    "    if seq_split > 2:\n",
    "        in_sq_0 = input_sequence[0][1:178] \n",
    "        in_sq_0 = torch.cat((element_to_prepend_2a3, in_sq_0, element_to_postpend_2a3))\n",
    "\n",
    "        out_2a3_0 = transformer_model(in_sq_0,tgt_sequence).to(dtype=torch.float16)\n",
    "\n",
    "\n",
    "        in_sq_1 = input_sequence[0][179:356]\n",
    "        in_sq_1 = torch.cat((element_to_prepend_2a3, in_sq_1, element_to_postpend_2a3))\n",
    "\n",
    "        out_2a3_1 = transformer_model(in_sq_1,tgt_sequence).to(dtype=torch.float16)\n",
    "\n",
    "\n",
    "        in_sq_2 = input_sequence[0][357:-1]\n",
    "        in_sq_2 = torch.cat((element_to_prepend_2a3, in_sq_2, element_to_postpend_2a3)) \n",
    "\n",
    "\n",
    "        padding = (179 - len(in_sq_2))\n",
    "        zeros_to_append = torch.zeros(padding).to(device=device, dtype=torch.long)\n",
    "        in_sq_2 = torch.cat((in_sq_2, zeros_to_append))\n",
    "\n",
    "        out_2a3_2 = transformer_model(in_sq_2,tgt_sequence).to(dtype=torch.float16)\n",
    "\n",
    "        ##############\n",
    "        in_sq_0 = input_sequence[0][1:178] \n",
    "        in_sq_0 = torch.cat((element_to_prepend_dms, in_sq_0, element_to_postpend_dms))\n",
    "\n",
    "        out_dms_0 = transformer_model(in_sq_0,tgt_sequence).to(dtype=torch.float16)\n",
    "\n",
    "\n",
    "        in_sq_1 = input_sequence[0][179:356]\n",
    "        in_sq_1 = torch.cat((element_to_prepend_dms, in_sq_1, element_to_postpend_dms))\n",
    "\n",
    "        out_dms_1 = transformer_model(in_sq_1,tgt_sequence).to(dtype=torch.float16)\n",
    "\n",
    "\n",
    "        in_sq_2 = input_sequence[0][357:-1]\n",
    "        in_sq_2 = torch.cat((element_to_prepend_dms, in_sq_2, element_to_postpend_dms)) \n",
    "\n",
    "\n",
    "        padding = (179 - len(in_sq_2))\n",
    "        zeros_to_append = torch.zeros(padding).to(device=device, dtype=torch.long)\n",
    "        in_sq_2 = torch.cat((in_sq_2, zeros_to_append))\n",
    "\n",
    "        out_dms_2 = transformer_model(in_sq_2,tgt_sequence).to(dtype=torch.float16)\n",
    "\n",
    "        print(f'{out_dms_0.size()} {out_dms_1.size()} {out_dms_2.size()} {id_minn} {id_maxx} {id_maxx - id_minn}')\n",
    "\n",
    "        iiii = 0\n",
    "        for iii in range(id_minn, id_minn + 178):\n",
    "            sample_df.at[iii, 'reactivity_2A3_MaP'] = out_2a3_0.tolist()[0][iiii]  \n",
    "            sample_df.at[iii, 'reactivity_DMS_MaP'] = out_dms_0.tolist()[0][iiii]\n",
    "            iiii = iiii + 1  \n",
    "\n",
    "        iiii = 0\n",
    "        for iii in range(id_minn + 179, id_minn + 179 + 178):\n",
    "            sample_df.at[iii, 'reactivity_2A3_MaP'] = out_2a3_1.tolist()[0][iiii]  \n",
    "            sample_df.at[iii, 'reactivity_DMS_MaP'] = out_dms_1.tolist()[0][iiii]\n",
    "            iiii = iiii + 1  \n",
    "\n",
    "        iiii = 0\n",
    "        for iii in range(id_minn + 179 + 179, id_maxx):\n",
    "            sample_df.at[iii, 'reactivity_2A3_MaP'] = out_2a3_2.tolist()[0][iiii]  \n",
    "            sample_df.at[iii, 'reactivity_DMS_MaP'] = out_dms_2.tolist()[0][iiii]\n",
    "            iiii = iiii + 1  \n",
    "        #print(\"here\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if seq_split <= 1:\n",
    "        #out_dms = []\n",
    "        out_dms = transformer_model(input_sequence,tgt_sequence).to(dtype=torch.float16)\n",
    "        out_dms[out_dms < 0] = 0.0\n",
    "        out_dms[out_dms > 1] = 1.0\n",
    "\n",
    "        out_dms = out_dms.tolist()   \n",
    "        #iiii = 0\n",
    "        #for iii in range(id_minn, id_maxx): \n",
    "        #    sample_df.at[iii, 'reactivity_DMS_MaP'] = out_dms[0][iiii]\n",
    "        #    sample_df.at[iii, 'id'] = iii\n",
    "        #    iiii = iiii + 1\n",
    "\n",
    "\n",
    "        #out_2a3 = []\n",
    "        input_sequence = torch.tensor([test_df.at[index, '2A3_sequence']]).to(device=device, dtype=torch.long)\n",
    "        out_2a3 = transformer_model(input_sequence,tgt_sequence).to(dtype=torch.float16)\n",
    "        out_2a3[out_2a3 < 0] = 0.0\n",
    "        out_2a3[out_2a3 > 1] = 1.0\n",
    "        out_2a3 = out_2a3.tolist()\n",
    "        #out_2a3[:seq_length]\n",
    "        #print(out_2a3)\n",
    "        #sample_df[id_minn:id_maxx]['reactivity_2A3_MaP'] = pd.DataFrame({'reactivity_2A3_MaP': out_2a3[0][:seq_length]}, index=range(id_minn, id_minn + seq_length))\n",
    "        #print(out_dms[0][25:25+176])\n",
    "        #print(out_2a3[0][25:25+176])\n",
    "\n",
    "        iiii = 0\n",
    "        for iii in range(id_minn, id_maxx):\n",
    "            sample_df.at[iii, 'reactivity_2A3_MaP'] = out_2a3[0][iiii]  \n",
    "            sample_df.at[iii, 'reactivity_DMS_MaP'] = out_dms[0][iiii]\n",
    "            iiii = iiii + 1  \n",
    "        \n",
    "    \n",
    "    \n",
    "    #print(sample_df['reactivity_2A3_MaP'].iloc[0])\n",
    "    #print(sample_df['reactivity_DMS_MaP'].iloc[0])\n",
    "    #break\n",
    "\n",
    "    #if index==2:\n",
    "    #    break\n",
    "    \n",
    "    \n",
    "    \n",
    "#sample_df   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc6a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "input_sequence = torch.tensor([test_df.at[335823, '2A3_sequence']]).to(device=device, dtype=torch.long)\n",
    "print(input_sequence[0][:179], input_sequence[0][:179].size(), id_minn, id_maxx, id_maxx-id_minn)\n",
    "\n",
    "\n",
    "seq_length = len(test_df.at[335823, 'sequence'])\n",
    "\n",
    "id_minn = test_df.at[335823, 'id_min']\n",
    "id_maxx = test_df.at[335823, 'id_max']\n",
    "\n",
    "seq_split = seq_length/179\n",
    "print(\"seq split\", seq_split)\n",
    "element_to_prepend_2a3 = torch.tensor([7]).to(device=device, dtype=torch.long)\n",
    "element_to_postpend_2a3 = torch.tensor([8]).to(device=device, dtype=torch.long)\n",
    "\n",
    "element_to_prepend_dms = torch.tensor([5]).to(device=device, dtype=torch.long)\n",
    "element_to_postpend_dms = torch.tensor([6]).to(device=device, dtype=torch.long)\n",
    "\n",
    "if seq_split > 1 and seq_split < 2:\n",
    "    in_sq_0 = input_sequence[0][1:178] \n",
    "    in_sq_0_2a3 = torch.cat((element_to_prepend_2a3, in_sq_0, element_to_postpend_2a3))\n",
    "    out_2a3_0 = transformer_model(in_sq_0_2a3,tgt_sequence).to(dtype=torch.float16)\n",
    "    \n",
    "    in_sq_0_dms = torch.cat((element_to_prepend_dms, in_sq_0, element_to_postpend_dms))\n",
    "    out_dms_0 = transformer_model(in_sq_0_dms,tgt_sequence).to(dtype=torch.float16)\n",
    "    \n",
    "    \n",
    "    \n",
    "    in_sq_1 = input_sequence[0][179:-1]\n",
    "    \n",
    "    in_sq_1_2a3 = torch.cat((element_to_prepend_2a3, in_sq_1, element_to_postpend_2a3))\n",
    "\n",
    "    \n",
    "    \n",
    "    padding = (179 - len(in_sq_1_2a3))\n",
    "    zeros_to_append = torch.zeros(padding).to(device=device, dtype=torch.long)\n",
    "    in_sq_1_2a3 = torch.cat((in_sq_1_2a3, zeros_to_append))\n",
    "    \n",
    "    out_2a3_1 = transformer_model(in_sq_1_2a3,tgt_sequence).to(dtype=torch.float16)\n",
    "    \n",
    "    in_sq_1_dms = torch.cat((element_to_prepend_dms, in_sq_1, element_to_postpend_dms))    \n",
    "    in_sq_1_dms = torch.cat((in_sq_1_dms, zeros_to_append))\n",
    "    out_dms_1 = transformer_model(in_sq_1_dms,tgt_sequence).to(dtype=torch.float16)\n",
    "    \n",
    "    iiii = 0\n",
    "    for iii in range(id_minn, id_minn + 178):\n",
    "        sample_df.at[iii, 'reactivity_2A3_MaP'] = out_2a3_0.tolist()[0][iiii]  \n",
    "        sample_df.at[iii, 'reactivity_DMS_MaP'] = out_dms_0.tolist()[0][iiii]\n",
    "        iiii = iiii + 1  \n",
    "    \n",
    "    iiii = 0\n",
    "    for iii in range(id_minn + 179, id_maxx):\n",
    "        sample_df.at[iii, 'reactivity_2A3_MaP'] = out_2a3_1.tolist()[0][iiii]  \n",
    "        sample_df.at[iii, 'reactivity_DMS_MaP'] = out_dms_1.tolist()[0][iiii]\n",
    "        iiii = iiii + 1 \n",
    "    \n",
    "    print(\"1 here\")\n",
    "    \n",
    "    \n",
    "if seq_split > 2:\n",
    "    in_sq_0 = input_sequence[0][1:178] \n",
    "    in_sq_0 = torch.cat((element_to_prepend_2a3, in_sq_0, element_to_postpend_2a3))\n",
    "    \n",
    "    out_2a3_0 = transformer_model(in_sq_0,tgt_sequence).to(dtype=torch.float16)\n",
    "\n",
    "    \n",
    "    in_sq_1 = input_sequence[0][179:356]\n",
    "    in_sq_1 = torch.cat((element_to_prepend_2a3, in_sq_1, element_to_postpend_2a3))\n",
    "    \n",
    "    out_2a3_1 = transformer_model(in_sq_1,tgt_sequence).to(dtype=torch.float16)\n",
    "\n",
    "    \n",
    "    in_sq_2 = input_sequence[0][357:-1]\n",
    "    in_sq_2 = torch.cat((element_to_prepend_2a3, in_sq_2, element_to_postpend_2a3)) \n",
    "    \n",
    "    \n",
    "    padding = (179 - len(in_sq_2))\n",
    "    zeros_to_append = torch.zeros(padding).to(device=device, dtype=torch.long)\n",
    "    in_sq_2 = torch.cat((in_sq_2, zeros_to_append))\n",
    "    \n",
    "    out_2a3_2 = transformer_model(in_sq_2,tgt_sequence).to(dtype=torch.float16)\n",
    "    \n",
    "    ##############\n",
    "    in_sq_0 = input_sequence[0][1:178] \n",
    "    in_sq_0 = torch.cat((element_to_prepend_dms, in_sq_0, element_to_postpend_dms))\n",
    "    \n",
    "    out_dms_0 = transformer_model(in_sq_0,tgt_sequence).to(dtype=torch.float16)\n",
    "\n",
    "    \n",
    "    in_sq_1 = input_sequence[0][179:356]\n",
    "    in_sq_1 = torch.cat((element_to_prepend_dms, in_sq_1, element_to_postpend_dms))\n",
    "    \n",
    "    out_dms_1 = transformer_model(in_sq_1,tgt_sequence).to(dtype=torch.float16)\n",
    "\n",
    "    \n",
    "    in_sq_2 = input_sequence[0][357:-1]\n",
    "    in_sq_2 = torch.cat((element_to_prepend_dms, in_sq_2, element_to_postpend_dms)) \n",
    "    \n",
    "    \n",
    "    padding = (179 - len(in_sq_2))\n",
    "    zeros_to_append = torch.zeros(padding).to(device=device, dtype=torch.long)\n",
    "    in_sq_2 = torch.cat((in_sq_2, zeros_to_append))\n",
    "    \n",
    "    out_dms_2 = transformer_model(in_sq_2,tgt_sequence).to(dtype=torch.float16)\n",
    "    \n",
    "    print(f'{out_dms_0.size()} {out_dms_1.size()} {out_dms_2.size()} {id_minn} {id_maxx} {id_maxx - id_minn}')\n",
    "    \n",
    "    iiii = 0\n",
    "    for iii in range(id_minn, id_minn + 178):\n",
    "        sample_df.at[iii, 'reactivity_2A3_MaP'] = out_2a3_0.tolist()[0][iiii]  \n",
    "        sample_df.at[iii, 'reactivity_DMS_MaP'] = out_dms_0.tolist()[0][iiii]\n",
    "        iiii = iiii + 1  \n",
    "    \n",
    "    iiii = 0\n",
    "    for iii in range(id_minn + 179, id_minn + 179 + 178):\n",
    "        sample_df.at[iii, 'reactivity_2A3_MaP'] = out_2a3_1.tolist()[0][iiii]  \n",
    "        sample_df.at[iii, 'reactivity_DMS_MaP'] = out_dms_1.tolist()[0][iiii]\n",
    "        iiii = iiii + 1  \n",
    "\n",
    "    iiii = 0\n",
    "    for iii in range(id_minn + 179 + 179, id_maxx):\n",
    "        sample_df.at[iii, 'reactivity_2A3_MaP'] = out_2a3_2.tolist()[0][iiii]  \n",
    "        sample_df.at[iii, 'reactivity_DMS_MaP'] = out_dms_2.tolist()[0][iiii]\n",
    "        iiii = iiii + 1  \n",
    "    print(\"here\")\n",
    "        \n",
    "        \n",
    "#out_2a3 = transformer_model(in_sq_1,tgt_sequence).to(dtype=torch.float16)\n",
    "#print(out_2a3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408df1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_sequence[0][179:-1]\n",
    "#index\n",
    "#print(f'{index} {id_minn} {id_maxx} {id_maxx - id_minn} {459 - 179*2} {179*2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a96b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DONe\")\n",
    "sample_df['id'] = sample_df['id'].astype(int)\n",
    "columns_order = ['id','reactivity_DMS_MaP','reactivity_2A3_MaP']\n",
    "out_sample_df = sample_df[columns_order]\n",
    "out_sample_df.to_csv(SUBMISSION_PATH, index=False,float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e294ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_df[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e50038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df['Seq_Length'] = test_df['2A3_sequence'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394eaf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_df['Seq_Length'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503036a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(test_df['2A3_sequence'][1343821])\n",
    "#len(test_df['2A3_sequence'][335823])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f45f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_2a3_0.tolist()[0][9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a8b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_df.at[1343821, 'reactivity_2A3_MaP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21757ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
