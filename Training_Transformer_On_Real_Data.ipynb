{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e3ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "#import polars as pl\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "\n",
    "import math\n",
    "import copy\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#Training Data\n",
    "df = pd.read_csv('/media/spartans/COMMON:/RNA_Project/train_data.csv')\n",
    "#df = pd.read_csv('/media/spartans/COMMON:/RNA_Project/small_train_data.csv')\n",
    "\n",
    "\n",
    "# Create a list of random data samples (for demonstration)\n",
    "max_seq_len = 206 + 2  # +2 is added to cover start and end token of sequence This will change to 457 in private set  # Set the maximum sequence length to 100\n",
    "\n",
    "\n",
    "seed=69\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966405ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        #print(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward1(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "    \n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        #print(f\"x after first linear layer: {x.size()}\")\n",
    "        x = self.relu(x)\n",
    "        #print(f\"x after activation: {x.size()}\")\n",
    "        x = self.dropout(x)\n",
    "        #print(f\"x after dropout: {x.size()}\")\n",
    "        x = self.linear2(x)\n",
    "        #print(f\"x after 2nd linear layer: {x.size()}\")\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0924687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        #print(self.pe[:, :x.size(1)].size())\n",
    "        #print((x + self.pe[:, :x.size(1)]).size())\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "    \n",
    "\n",
    "class PositionalEncoding1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        print(\"PE size\", PE.size())\n",
    "        print(\"x size\", x.size())\n",
    "        print(\"PE x size\", PE[:, :x.size(1)].size())\n",
    "        print(\"stacked dim\", stacked.size())\n",
    "        return x + PE[:, :x.size(1)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a50616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13abad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd6280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(device)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        #print(\"forward \",src, src.size())\n",
    "        #print(\"forward \", tgt, tgt.size())\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        #include here FC layers with output dimension = input dimension of the sequence (considering padding this will be const number)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d590ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6e9864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#src_data\n",
    "#src_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f1504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#src_data.size()\n",
    "#src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771670e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb25d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#where reactivity is NaN,\n",
    "# have Nan replaced with average of all numbers in the sequence.\n",
    "\n",
    "#Map sequence letters to numbers.\n",
    "\n",
    "#Looking at Reads and Signal To Noise ..they appear to be somewhat coorelated.\n",
    "\n",
    "#Create Dataset function\n",
    "\n",
    "\n",
    "#In contrastive loss model... you can train based on#\n",
    "#in a sequence which part is 2D fold and which part is 3D fold\n",
    "#See notebook https://www.kaggle.com/code/something4kag/ribonanza-3d-coords-prep\n",
    "#if it can be helpful\n",
    "# Also analyze and which position in the sequence the fold can occur and which fold ended up happening based on the data\n",
    "\n",
    "#another method to formulate contrastive learning model is by deferentiating sequences with low SNR and hence reactivity value will not be with high confidence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df464413",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#In RNA, the most common base pairings you'll find are between the following nucleotide bases:\n",
    "\n",
    "#Adenine (A) and Uracil (U): A forms base pairs with U. This is a fundamental pairing and is commonly seen in RNA molecules, particularly in single-stranded regions. It is important for the stability of stem-loop structures and is a key component of RNA secondary structure.\n",
    "\n",
    "#Guanine (G) and Cytosine (C): G forms base pairs with C, just as it does in DNA. This pairing is less common in RNA secondary structure but is still important for certain RNA molecules. It may be more prevalent in the context of ribozymes and catalytic RNA.\n",
    "\n",
    "#While A-U and G-C are the primary base pairings in RNA, it's essential to understand that RNA can also exhibit non-canonical or non-standard base pairings, especially in more complex RNA structures. These non-canonical pairings can involve different combinations of A, U, G, and C and are often seen in tertiary structures or specialized RNA molecules with specific functions.\n",
    "\n",
    "#The prevalence of A-U and G-C base pairings in an RNA molecule can vary depending on its sequence and function. For instance, regions that need to form stable secondary structures often rely on A-U pairings, while regions involved in catalytic activities may include G-C pairs. RNA structures are diverse and can exhibit a wide range of base pairing interactions to achieve their biological functions.\n",
    "\n",
    "\n",
    "\n",
    "#So a contrastive loss can be formed here where\n",
    "#AU pair GC pair are strong positive - stable\n",
    "#AG, UC are less stable\n",
    "#UG, UA, CG pair are unstable so negative .\n",
    "#\n",
    "#In RNA, the stable base pairs among the four nucleotide bases (A, G, U, and C) follow the standard Watson-Crick base-pairing rules. Here are the stable base pairs among these bases:\n",
    "\n",
    "#GC pairs are more stable than AU pair\n",
    "\n",
    "\n",
    "#So you can tokenzie these pairs in the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7140ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formulate Convolution NN or Dense Net which takes SNR, Reactivity error and feeds into the final layer of Transformer network\n",
    "# As an example\n",
    "#Lowest Reactivity Error and High SNR are  strong positives\n",
    "# Lowest SNR are strong negative.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#sequence length range from 115 to 206 in train dataset.\n",
    "#in final it will range from 207 to 457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d38e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In positional encoding\n",
    "#pos means the position of the word in the sequence.\n",
    "#i means the index for that particular word vector (index of the dimension)\n",
    "#dmodel is the dimension for e.g 512 from the example\n",
    "\n",
    "\n",
    "#For 2 experiment types i.e. DMS and 2A3 you can have start and end  token in a sequence for which data is available.\n",
    "#.e. start_DMS, end_DMS, start_2A3, end_2A3.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf8b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.iloc[:,150:213]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e535ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a703310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning reactivity Columns to react_columns dataframe and replacing NaN with 0.000 \n",
    "\n",
    "#del(mean_reactivity)\n",
    "#del(react_columns_New)\n",
    "#del(react_columns)\n",
    "react_columns = df.iloc[:,7:213].fillna(0.000000)\n",
    "\n",
    "#Find mean of reactivity per row\n",
    "#df['mean'] = df.mean(axis=1)\n",
    "react_columns_mean = react_columns.mean(axis=1)\n",
    "#Free up memory\n",
    "del(react_columns)\n",
    "\n",
    "#Now replace NaN in react_columns with mean_reactivity\n",
    "#react_columns_New = df.iloc[:,7:213].fillna(mean_reactivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86be9980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "react_columns = df.iloc[:,7:213] #You may have to capture those indices as per sequence length i.e. when it extends to 457\n",
    "\n",
    "# Replace NaN values in each row with values from replace_values array\n",
    "for index, row in tqdm(react_columns.iterrows(), total=len(react_columns), desc=\"Processing rows\"):\n",
    "    react_columns.loc[index] = row.fillna(react_columns_mean[index])\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b09780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#react_columns_New = df.iloc[:,7:213].fillna(react_columns['mean_reactivity'])\n",
    "#react_columns_New\n",
    "#react_columns.columns\n",
    "\n",
    "\n",
    "#for column_name in tqdm(react_columns.columns, total=len(react_columns.columns), desc=\"Processing\"):\n",
    "#    df[column_name] = react_columns[column_name]\n",
    "    \n",
    "#react_columns['sequence'] = df['experiment_type'] + 'start' + df['sequence']\n",
    "#react_columns['sequence'] = react_columns['sequence'] + 'end' + df['experiment_type']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0046f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_map = {'A':1,\n",
    "           'C':2,\n",
    "           'G':3,\n",
    "           'U':4, \n",
    "           'M':5,  #Start token of DMS exp seq\n",
    "           'N':6,  #End token of DMS exp seq\n",
    "           'T':7,  #Start token of 2A3 exp seq\n",
    "           'X':8,  #End token of 2A3 exp seq\n",
    "           'Z':0   #Padding token\n",
    "          }\n",
    "#for s in df.at[0,'sequence']:\n",
    "#    print(seq_map[s])\n",
    "#df\n",
    "\n",
    "#test_cell = [seq_map[s] for s in df.at[0, 'sequence']]\n",
    "#print(test_cell)\n",
    "\n",
    "#len(df.at[0, 'sequence'])\n",
    "# Create a list of random data samples (for demonstration)\n",
    "#max_seq_len = 206 #This will change to 457 in private set  # Set the maximum sequence length to 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e4ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over rows and replace a specific value in all columns\n",
    "\n",
    "#seq = [self.seq_map[s] for s in seq]\n",
    "#for index, row in df.iterrows():\n",
    "# Create a list of random data samples (for demonstration)\n",
    "#max_seq_len = 206 #This will change to 457 in private set  # Set the maximum sequence length to 100\n",
    "\n",
    "df_sq = copy.deepcopy(df['sequence']) #saving a backup of sequences.\n",
    "#react_columns['maped_sequence'] = [' ']*len(react_columns)\n",
    "#for index, row in tqdm(df.iterrows(), total=len(df.rows), desc=\"Processing rows\"):\n",
    "for index, row in tqdm(react_columns.iterrows(), total=len(react_columns), desc=\"Processing rows\"):\n",
    "    if df.at[index, 'experiment_type'] == \"2A3_MaP\":\n",
    "        concat_seq = 'T' + df.at[index, 'sequence'] + 'X'\n",
    "    if df.at[index, 'experiment_type'] == \"DMS_MaP\":\n",
    "        concat_seq = 'M' + df.at[index, 'sequence'] + 'N'\n",
    "    padding = \"Z\"*(max_seq_len - 2 - len(df.at[index, 'sequence']))\n",
    "    concatenated_seq = concat_seq + padding\n",
    "    df.at[index, 'sequence'] = [seq_map[s] for s in concatenated_seq]\n",
    "    #react_columns.at[index, 'maped_sequence'] = [seq_map[s] for s in concatenated_seq]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6aebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(df.at[0, 'sequence'])\n",
    "#df\n",
    "#react_columns['maped_sequence'] = [' ']*len(react_columns)\n",
    "\n",
    "\n",
    "react_columns['reactivity_0207'] = copy.deepcopy(react_columns['reactivity_0206'])\n",
    "react_columns['reactivity_0208'] = copy.deepcopy(react_columns['reactivity_0206'])\n",
    "react_columns['mapped_sequence'] = copy.deepcopy(df['sequence'])\n",
    "#del react_columns['reactivity_0207']\n",
    "#del react_columns['reactivity_0208']\n",
    "react_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c589ca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#react_columns.select_dtypes(exclude=\"NaN\")\n",
    "#df[~df['COLUMN1'].str.contains('TOTAL')]\n",
    "#react_columns.iloc[~react_columns[:,1].str.contains('NaN')]\n",
    "\n",
    "#react_columns.iloc[:,1]\n",
    "\n",
    "#data1 = [[1, 1, 2], [6, 4, 2], [4, 2, 1], [4, 2, 3]]\n",
    "\n",
    "#daf = pd.DataFrame(data1)\n",
    "#print(data1)\n",
    "#print(daf.mean())\n",
    "#react_columns['sequence'][150:170][160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a5fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for your data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, max_seq_len):\n",
    "        self.data = data\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #src_sequence, trg_sequence = [0,1] #self.data[idx]\n",
    "        #src_sequence = self.data['reactivity_0001']\n",
    "        #trg_sequence = self.data['mapped_sequence']\n",
    "        #list_src_trg_seq = self.data.loc[idx].apply(lambda row: row.tolist(), axis=1)\n",
    "        #merged_values = ','.join(map(str, df.loc[row_index_to_merge][:-1]))\n",
    "        #list_src_trg_seq = ','.join(map(str, self.data.loc[row_index_to_merge][:-1]))\n",
    "        #src_sequence = list_src_trg_seq[0:-1]\n",
    "        #srg_sequence = list_src_trg_seq[-1]\n",
    "        \n",
    "        #trg_sequence = ','.join(map(str, self.data.loc[idx][:-1]))\n",
    "        #src_sequence = [','.join(map(str, self.data.loc[idx][-1]))]\n",
    "        \n",
    "        trg_sequence = torch.tensor(self.data.iloc[idx, :].tolist()[:-1])\n",
    "        src_sequence = torch.tensor(self.data.iloc[idx, :].tolist()[-1])\n",
    "        \n",
    "        #print(\"Size src seq \", src_sequence.size())\n",
    "        #print(\"Size trg seq \", trg_sequence.size())\n",
    "        \n",
    "\n",
    "        #trg_sequence = self.data.iloc[idx, :].totensor()[:-1]\n",
    "        #src_sequence = self.data.iloc[idx, :].totensor()[-1]\n",
    "        \n",
    "        # Pad sequences with zeros to match the length of the longest sequence in each batch\n",
    "        #max_len = max(len(src_sequence), len(trg_sequence), self.max_seq_len)\n",
    "        #src_sequence += [0] * (max_len - len(src_sequence))\n",
    "        #trg_sequence += [0] * (max_len - len(trg_sequence))\n",
    "        #print(\"Index \", idx, \" SRC--\", src_sequence, \" TRG--\", trg_sequence)\n",
    "        #print(\"SRC LEN \", len(src_sequence), \" TRG LEN \", len(trg_sequence))\n",
    "        return src_sequence, trg_sequence\n",
    "\n",
    "\n",
    "\n",
    "#data = []\n",
    "#for _ in range(5):\n",
    "#    src_sequence = [random.randint(1, 100) for _ in range(random.randint(5, max_seq_len))]\n",
    "#    trg_sequence = [random.randint(1, 100) for _ in range(random.randint(5, max_seq_len))]\n",
    "#    data.append((src_sequence, trg_sequence))\n",
    "\n",
    "#print(\"src\", len(src_sequence))\n",
    "#print(\"trg\", len(trg_sequence))\n",
    "\n",
    "#list_src_trg_seq = react_columns.apply(lambda row: row.tolist(), axis=1)\n",
    "\n",
    "    \n",
    "# Create a DataLoader for batching and shuffling\n",
    "batch_size = 64\n",
    "#batch_size = 3\n",
    "\n",
    "\n",
    "# Define the size of the training set\n",
    "dataset_size = len(react_columns)\n",
    "train_size = int(0.9 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "# Create indices for training and testing sets\n",
    "#indices = list(range(dataset_size))\n",
    "#train_indices, test_indices = random_split(indices, [train_size, test_size])\n",
    "#print(train_indices)\n",
    "#print(test_indices)\n",
    "custom_dataset = CustomDataset(react_columns, max_seq_len)\n",
    "train_indices, test_indices = random_split(custom_dataset, [train_size, test_size])\n",
    "train_dataloader = DataLoader(train_indices, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_indices, batch_size=batch_size)\n",
    "\n",
    "\n",
    "  \n",
    "# Example usage in the training loop (as previously shown)\n",
    "#for batch in dataloader:\n",
    "#    print(\"Inside DataLoader Func\")\n",
    "#    src, trg = batch\n",
    "#    print(\"SRC -\", src, src.size())\n",
    "#    print(\"TRG -\", trg, trg.size())\n",
    "    # ...\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "#tgt_vocab_size = 5000\n",
    "tgt_vocab_size = 206\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = max_seq_len\n",
    "dropout = 0.1\n",
    "\n",
    "# Generate random sample data\n",
    "#src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)\n",
    "#tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7702c7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "epochx_train =[]\n",
    "epochx_test =[]\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(3):\n",
    "    for src_data, tgt_data in train_dataloader:\n",
    "        #    print(\"Inside DataLoader Func\")\n",
    "        #src_data, trg_data = src_data.to(device), trg_data.to(device)\n",
    "        #print(\"SRC -\", src, src.size())\n",
    "        #print(\"TRG -\", trg, trg.size())\n",
    "        #src_data = torch.tensor(src_data)\n",
    "        #trg_data = torch.tensor(trg_data)\n",
    "        src_data, tgt_data = src_data.to(device), tgt_data.to(device,  dtype=torch.long)\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        transformer.train()\n",
    "        optimizer.zero_grad()\n",
    "        #src_data = src_data.to(device)\n",
    "        #x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "        #y = y.to(device=device, dtype=torch.long)\n",
    "        #tgt_data = tgt_data.to(device)\n",
    "        #print(\"TRG DATA \", tgt_data.size())\n",
    "        output = transformer(src_data, tgt_data[:, :-1])\n",
    "        #output = transformer(src_data, tgt_data)\n",
    "        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch: {epoch+1}, Training Loss: {loss.item()}\")\n",
    "        train_loss += loss.item()\n",
    "        train_losses.append(train_loss / len(src_data))\n",
    "        epochx_train.append(epoch+1)\n",
    "        #validation part\n",
    "        #Restructure code with validation set data\n",
    "        transformer.eval()\n",
    "        with torch.no_grad():\n",
    "            for src_data, tgt_data in test_dataloader:\n",
    "                #This should be validation set data.\n",
    "                src_data = src_data.to(device=device)\n",
    "                tgt_data = tgt_data.to(device=device, dtype=torch.long)\n",
    "                output = transformer(src_data, tgt_data[:, :-1])\n",
    "                loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "                print(f\"Epoch: {epoch+1}, Validation Loss: {loss.item()}\")\n",
    "                val_loss += loss.item()\n",
    "                val_losses.append(val_loss / len(src_data))\n",
    "                epochx_test.append(epoch+1)\n",
    "        \n",
    "\n",
    "torch.save(transformer, \"Trained_Model.model\")\n",
    "\n",
    "#print(epochx_train, len(epochx_train))\n",
    "#print(train_losses, len(train_losses))\n",
    "# Plot both training and validation losses\n",
    "#plt.bar(epochx_train, height=0)\n",
    "plt.plot(epochx_train, train_losses, label='Training Loss')\n",
    "plt.plot(epochx_test, val_losses,  label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6f28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754c983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
